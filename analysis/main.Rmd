---
title: "Data Analysis for Business, Economics and Policy"
subtitle: "Case studies"
author: "Fernando Millan Villalobos"
date: "July 2021"
output:
  html_document:
    code_folding: show
    echo: TRUE
    warning: FALSE
    message: FALSE
    highlight: pygments
    theme: paper
    wms_print: kable
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float: 
      collapsed: yes
      smooth_scroll: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../", output_file = "index") })     
---

```{r, include=FALSE}
## By default, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE
)
```

```{r, echo=FALSE}
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "dabep" # adapt!
package_date <- "2021-06-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.1.1" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
  stop("ERROR: specified R version does not match currently used.")
}
```

# Notes

This report was generated on `r Sys.time()`. R version: `r paste0(version$major, ".", version$minor)` on `r version$platform`. For this report, CRAN packages as of `r package_date` were used.

...

## R-Script & Data

The preprocessing and analysis of the hotels_raw was conducted in the [R project for statistical computing](https://www.r-project.org/). The RMarkdown script used to generate this document and all the resulting hotels_raw can be downloaded [under this link](http://%60r%20user_name%60.github.io/%60r%20project_name%60/). Through executing `main.Rmd`, the herein described process can be reproduced and this document can be generated. In the course of this, hotels_raw from the folder `input` will be processed and results will be written to `output`. The html on-line version of the analysis can be accessed through this [link](https://%60r%20user_name%60.github.io/%60r%20project_name%60/).

## GitHub

The code for the herein described process can also be freely downloaded from [https://github.com/`r user_name`/`r project_name`](https://github.com/%60r%20user_name%60/%60r%20project_name%60).

## License

...

## Data description of output files

#### `abc.csv` (Example)

| Attribute | Type    | Description |
|-----------|---------|-------------|
| a         | Numeric | ...         |
| b         | Numeric | ...         |
| c         | Numeric | ...         |

#### `xyz.csv`

...

# Set up

```{r, echo=FALSE}
detach_all_packages <- function() {
  basic_packages_blank <- c(
    "stats",
    "graphics",
    "grDevices",
    "utils",
    "hotels_rawsets",
    "methods",
    "base"
  )
  basic_packages <- paste("package:", basic_packages_blank, sep = "")

  package_list <- search()[
    ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)
  ]

  package_list <- setdiff(package_list, basic_packages)

  if (length(package_list) > 0) {
    for (package in package_list) {
      detach(package, character.only = TRUE, unload = TRUE)
      print(paste("package ", package, " detached", sep = ""))
    }
  }
}

detach_all_packages()

# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
  print("WARNING: No working directory specified for current user")
} else {
  setwd(path_to_wd)
}

# suppress scientific notation
options(scipen = 999)

# suppress summarise info
options(dplyr.summarise.inform = FALSE)

# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
  detach_all_packages()
}
```

## Define packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse, warn.conflicts = FALSE) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(jsonlite) # json
library(lintr) # code linting
library(sf)
library(rmarkdown)
library(data.table)
library(cowplot) # theme
library(extrafont)
library(waldo) # compare
library(psych) # some useful funs
library(ggrepel) # text labels
library(skimr)
library(haven)
library(Hmisc)
library(desc)
library(reshape2)
library(pastecs)
library(patchwork)
library(xtable)
library(reticulate)
library(binsreg)
library(boot)
library(modelsummary)
library(viridis)
library(grid)
library(janitor)", # names
  file = "manifest.R"
)
```

## Install packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
  if (!require(devtools)) {
    install.packages("devtools", repos = "http://cran.us.r-project.org")
    require(devtools)
  }
  devtools::install_github("RevolutionAnalytics/checkpoint",
    ref = "v0.3.2", # could be adapted later,
    # as of now (beginning of July 2017
    # this is the current release on CRAN)
    repos = "http://cran.us.r-project.org"
  )
  require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
  dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(
  snapshot_date = package_date,
  project = path_to_wd,
  verbose = T,
  scanForPackages = T,
  use.knitr = F,
  R.version = r_version
)
rm(package_date)
```

## Load packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
source("manifest.R")
unlink("manifest.R")
sessionInfo()
```

## Load additional scripts

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if you want to outsource logic to other script files, see README for
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
knitr::read_chunk("scripts/outliers.R")
source("scripts/outliers.R")
knitr::read_chunk("scripts/theme_bg.R")
source("scripts/theme_bg.R")
```

# CHAPTER 02: Preparing Data for Analysis

## Entity Resolution: Duplicates, Ambiguous Identification, and Non-entity Rows

One potential issue is having **duplicate observations** in the data table. In the simplest case, duplicates are perfect: the value of all variables is the same. In such cases we have to delete duplicates and leave one data row for each observation only.

A related, but conceptually different issue is to have **ambiguous identification**: the same entity having different IDs across different tables. The task here is to make sure that each entity has the same ID across data tables. That is necessary to link them properly. Entities are frequently identified by names. Unfortunately, though, names may cause issues for two main reasons: they are not unique, and different data tables may have different versions of the same name. This task is called **disambiguation**: making identification of entities not ambiguous.

Yet another issue is having non-entity observations: rows that do not belong to an entity we want in the data table. Before any meaningful analysis can be done, we need to erase such rows from the data table. The important message is this: *assigning unique IDs is important and using numerical ID variables is good practice*.

## Managing Missing Values

Having missing values for some variables is a frequent problem. There are two different main approaches: First, we can work with observations that have non-missing values for all variables used in the analysis. This is the most natural and most common approach. One version of this approach is to work with observations that make up a well-defined subsample, in which the missing data problem is a lot less severe. The second option is filling in some value for the missing values, such as the average value. This is called **imputation**. When missing values are replaced with some other values, it is good practice to create a binary variable that indicates that the original value was missing. Such a variable is called a **flag**.

When possible, focus on more fully filled variables. Imputation for qualitative variables should be done differently. We should add an additional category for missing values. For ordinal variables, we may also impute the median category and add a new variable denoting that the original value was missing.

## Organizing Data Tables for a Project

It is good practice to organize and store the data at three levels:

-   Raw data tables
-   Clean and tidy data tables
-   Workfile for analysis

**Raw data** may come in a single or in multiple files. It should be stored in the original format before anything is done to it. The next step is producing **clean** and **tidy** data from raw data. Often tidy data means multiple data tables. We should always create different data tables for data with different kinds of observations. The last of the three levels is the one on which the analysis is to be done: the **workfile**.

Project overview:

![Project Overview](input/project_organization.png)

```{r ch02_hotels}
# load hotels_raw
hotels_raw <- read_csv("input/hotelbookingdata-vienna.csv")
# load hotels_clean
hotels_clean <- read_csv("input/hotels-vienna.csv")

# transform values of some variables (through splitting) into valid values
hotels_raw <- hotels_raw |>
  tidyr::separate(center1distance, c("distance", NA), sep = " ") |>
  tidyr::separate(center2distance, c("distance_alter", NA), sep = " ") |>
  tidyr::separate(accommodationtype, c(NA, "accommodation_type"), sep = "@") |>
  tidyr::separate(price_night, c(NA, NA, "nnight", NA), sep = " ") |>
  tidyr::separate(guestreviewsrating, c("rating", NA), sep = " ")
head(hotels_raw)

# check: frequency table of all values incl. missing values
tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating_reviewcount <- hotels_raw |>
  dplyr::group_by(rating_reviewcount) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

hotels_raw <- hotels_raw |>
  dplyr::mutate(rating_count = as.numeric(rating_reviewcount))

Hmisc::describe(hotels_raw$rating_count)

# rename variables
hotels_raw <- hotels_raw |>
  dplyr::rename(
    ratingta = rating2_ta,
    ratingta_count = rating2_ta_reviewcount,
    country = addresscountryname,
    city = s_city, stars = starrating
  )


# check: key variables
tab_stars <- hotels_raw |>
  dplyr::group_by(stars) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

# look for perfect duplicates
hotels_raw <- hotels_raw |>
  dplyr::arrange(hotel_id)

# filtering duplicates rows by IDs and selecting key variables to check
hotels_raw |>
  dplyr::group_by(hotel_id) |>
  dplyr::filter(n() > 1) |>
  dplyr::select(c(hotel_id, accommodation_type, price, distance, stars, rating, rating_count))

# getting rid of all duplicates (perfect duplicates)
hotels_raw <- hotels_raw |>
  dplyr::distinct()

# handling missing values in text
# checking NAs values
summary(hotels_raw)

# making a summary of the missing values
summary_wms <- t(stat.desc(hotels_raw))
summary_wms

# creating a flag variable for missing values
hotels_raw <- hotels_raw |>
  dplyr::mutate(misrating = ifelse(is.na(rating), 1, 0))

# counting all missing values
table(hotels_raw$misrating)

# looking missing values number per accomodation type
addmargins(table(hotels_raw$accommodation_type, hotels_raw$misrating))

# looking at the relation between missing values and price (mean)
hotels_raw |>
  dplyr::group_by(accommodation_type, misrating) |>
  dplyr::summarise(mean(price))

# spotting the exceptional case for "Hotels"
hotels_raw |>
  dplyr::filter((misrating == 1) & (accommodation_type == "Hotel")) |>
  dplyr::select(hotel_id, accommodation_type, price, distance, stars, rating, rating_count) |>
  dplyr::slice(1)
```

## Finding the Most Successful Managers

```{r ch02_football}
# load data
games <- read_csv("input/epl_games.csv")
team_games <- read_csv("input/epl-teams-games.csv")
managers <- read_csv("input/football_managers.csv")
merged <- read_csv("input/football_managers_workfile.csv")

dplyr::glimpse(games)
dplyr::glimpse(team_games)
dplyr::glimpse(managers)
dplyr::glimpse(merged)

# look at basic data
games <- games |>
  dplyr::arrange(team_home)

games <- games |>
  dplyr::arrange(season, team_home)

games <- games |>
  dplyr::filter(season == 2016)

# at team-game level
team_games <- team_games |>
  dplyr::arrange(team)

team_games <- team_games |>
  dplyr::arrange(season, team)

team_games <- team_games |>
  dplyr::filter(season == 2016) |>
  dplyr::arrange(date)

Hmisc::describe(merged$manager_id)

# arranging per season and team
merged <- merged |>
  dplyr::arrange(season, team)

# considering average points per game as a measure of success
# 1. looking at the number of games managed per manager
games <- merged |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::summarise(manager_games = n())

# 2. adding up the points earned over a career at a team (if manager worked for two teams, we consider it two cases)
points <- merged |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::summarise(manager_points = sum(points))

# 3. once we have games and points, we divide total points by the number of games
avg_points <- merge(games, points, by = c("manager_id", "team", "manager_name")) |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::mutate(manager_avg_points = (manager_points / manager_games)) |>
  dplyr::arrange(manager_avg_points)

# 4. ranking the results
avg_points <- avg_points |>
  dplyr::arrange(-manager_avg_points)

# 5. looking at those with at least 2 points per game
top_managers <- avg_points |>
  dplyr::filter(manager_avg_points >= 2)

# 6. denoting caretakers (less than 18 games)
top_managers <- top_managers |>
  dplyr::mutate(
    manager_avg_points0 = ifelse(manager_games < 18, manager_avg_points, NA),
    manager_avg_points1 = ifelse(manager_games > 18, manager_avg_points, NA)
  )

# adding new variable for plotting
top_managers <- top_managers |>
  dplyr::mutate(fill = case_when(
    manager_games < 18 ~ "1",
    manager_games > 18 ~ "0"
  ))

# plotting the results
p1 <- top_managers |>
  ggplot(aes(x = reorder(manager_name, manager_avg_points), y = manager_avg_points, fill = fill, alpha = fill)) +
  geom_col(show.legend = F) +
  ylab("Average points per game") +
  xlab("Manager name") +
  scale_fill_manual(values = c(color[1], color[4])) +
  scale_alpha_manual(values = c(0.8, 0.3)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 3), breaks = seq(0, 3, 0.3)) +
  coord_flip() +
  theme_bg() +
  cowplot::background_grid(major = "x", minor = "none")
p1
```

# CHAPTER 03: Exploratory Data Analysis (EDA)

## Frequencies and Probabilities

The **absolute frequency**, or count, of a value of a variable is simply the number of observations with that particular value in the data. The **relative frequency** is the frequency expressed in relative, or percentage, terms: the proportion of observations with that particular value among all observations in the data. If a variable has missing values, this proportion can be relative to all observations including the missing values or only to observations with missing values.

**Probability** is a general concept that is closely related to relative frequency. Probability is a measure of the likelihood of an event. In the context of data, an event is *the occurrence of a particular value of a variable*. **The probability of an event is its relative frequency.** Probabilities are always between zero and one. We denote the probability of an event as $$P(event)$$, so that $$0 <= P(event) <= 1$$. Considering a single event, it either happens or does not. These two are mutually exclusive: the probability of an event happening and it also not happening is zero: $$P(event&~even) = 0$$.

## Visualizing Distributions

Frequencies are summarized by distributions. The **distribution** of a variable gives the frequency of each value of the variable in the data, either in terms of absolute frequency (number of observations), or relative frequency (proportion or percent).

The simplest way to visualize a distribution is the **histogram**. For variables with many potential values, showing bars for each value is usually uninformative. Instead, it's better to group the many values in fewer groupings or bins. Visual inspection of a histogram can inform us about the number and location of **modes**: these are peaks in the distribution that stand out from their immediate neighborhood.

**Density plots**, also called **kernel density estimates**, are an alternative to histograms for variables with many values.

## Extreme Values

What it should be do with observations with extreme values of a variable depends on the role of the variable in our analysis. We distinguish a *y* variable and one or more *x* variables in the analysis. Typically, our analysis will aim to uncover patterns in how values of *y* tend to differ for observations that have different values of *x*. **Discarding observations with extreme *x* values narrows the scope of the comparisons.** In contrast, discarding observations with extreme *y* values changes the result of the comparisons.

```{r distributions}
# load data
vienna <- read_csv("input/hotels-vienna.csv")

# looking at the frequencies of all types of accommodation
table(vienna$accommodation_type)

# filtering accommodation: Hotel
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel")

# plotting distributions
# Absolute frequency (count)
p1 <- ggplot(data = vienna_hotels, aes(x = stars)) +
  geom_bar(color = color.outline, fill = color[1], alpha = 0.8, na.rm = T) +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 2.5) +
  labs(x = "Star rating (N. stars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0.5, 5.5), breaks = seq(1, 5, 0.5)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 140), breaks = seq(0, 140, 20)) +
  theme_bg()

# Relative frequency (percent)
p2 <- ggplot(data = vienna_hotels, aes(x = stars, y = (..count..) / sum(..count..))) +
  geom_bar(color = color.outline, fill = color[1], alpha = 0.8, na.rm = T) +
  geom_text(stat = "count", aes(label = round((..count..) / sum(..count..) * 100, 1)), vjust = -0.5, size = 2.5) +
  labs(x = "Star rating (N. stars)", y = "Percent") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0.5, 5.5), breaks = seq(1, 5, 0.5)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.5), breaks = seq(0, 0.5, 0.1), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()

p1|p2

# filtering 3-4 stars, without 1000 euro extreme value
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(price <= 1000)

table(vienna_hotels$city)
table(vienna_hotels$stars)

# plotting hotel price (different bindwidth)
p3 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 10) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p4 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 40) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p5 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 80) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p6 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

(p3 | p4 | p5) / p6

# plotting distance
p7 <- ggplot(data = vienna_hotels, aes(x = distance)) +
  geom_histogram_da(type = "frequency", binwidth = 0.5) +
  labs(x = "Distance to city center (miles)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 14), breaks = seq(0, 14, by = 2)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 61), breaks = seq(0, 60, by = 10)) +
  geom_segment(aes(x = 8.2, y = 0, xend = 8.2, yend = 60), color = color[2], size = 1) +
  # geom_segment(aes(x = 10, y = 40, xend = 8.4, yend = 40), arrow = arrow(length = unit(0.2, "cm")))+
  annotate("text", x = 11, y = 29, label = "Too far out", size = 2) +
  annotate("rect", xmin = 8.2, xmax = 14, ymin = 0, ymax = 60, fill = color[4], alpha = 0.1) +
  theme_bg()
p7

# looking at the actual place of hotels (included outside the city of Vienna)
table(vienna_hotels$city_actual)

# filtering 3-4 stars, less than 8miles from center, without 1000 euro extreme value
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(price <= 1000) |> 
  dplyr::filter(city_actual == "Vienna")
```

## Summary Statistics for Quantitative Variables

While measures of central value (such as mean, median) and spread (such as range, standard deviation) are usually well known, summary statistics that measure **skewness** are less frequently used. A distribution may be skewed in two ways, having a long left tail or having a long right tail. A long left tail means having a few observations with large values with most observations having larger values. A long right tail means having a few observations with large values with most observations having smaller values. The statistic of skewness compares the mean and the median and is called the **mean-median measure of skewness**. When the distribution is symmetric, its mean and median are the same. When it is skewed with a long right tail, the mean is larger than the median. Conversely, when a distribution is skewed with a long left tail, the mean is smaller than the median. The mean-median measure of skewness captures this intuition. In order to make this measure comparable across various distributions, we use a standardized measure, dividing the difference by the standard deviation: $$ Skewness = \frac{\bar{x} - median[x]}{Std[x]}$$

```{r summary_statistics}
# CASE STUDY: Comparing Hotel Prices in Europe: Vienna vs London
# load data
hotels_europe_price <- read_csv("input/hotels-europe_price.csv")
hotels_europe_features <- read_csv("input/hotels-europe_features.csv")

# creating a working data table for analysis
hotels_europe <- dplyr::left_join(hotels_europe_price, hotels_europe_features, by = "hotel_id")

# filtering for same Vienna data we used + London same date
hotels_europe_london_viena <- hotels_europe |>
  dplyr::filter(year == 2017 & month == 11 & weekend == 0) |>
  dplyr::filter(city %in% c("Vienna", "London")) |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(city_actual %in% c("Vienna", "London")) |>
  dplyr::filter(price <= 600)

# looking at the mean price for both cities
hotels_europe_london_viena |>
  dplyr::group_by(city) |>
  dplyr::summarise(mean_price = mean(price), max = max(price), n = n())

# plotting the distribution of hotel price
p1 <- ggplot(data = filter(hotels_europe_london_viena, city == "Vienna"), aes(x = price)) +
  geom_histogram_da(type = "percent", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Percent", title = "Vienna") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.1), labels = scales::percent_format()) +
  theme_bg()

p2 <- ggplot(data = filter(hotels_europe_london_viena, city == "London"), aes(x = price)) +
  geom_histogram_da(type = "percent", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Percent", title = "London") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.1), labels = scales::percent_format()) +
  theme_bg()

(p1 | p2)

# plotting same data using density plot
p3 <- ggplot(data = hotels_europe_london_viena, aes(x = price, y = stat(density), color = city)) +
  geom_line(stat = "density", show.legend = F, na.rm = TRUE) +
  labs(x = "Price (US dollars)", y = "Density", color = "") +
  scale_color_manual(
    name = "",
    values = c(color[2], color[1]),
    labels = c("London", "Vienna")
  ) +
  scale_y_continuous(expand = c(0.0, 0.0), limits = c(0, 0.015), breaks = seq(0, 0.015, by = 0.003)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  geom_text(aes(x = 340, y = 0.0026, label = "London"), color = color[2], size = 2.5) +
  geom_text(aes(x = 170, y = 0.008, label = "Vienna"), color = color[1], size = 2.5) +
  theme_bg()
p3

# summarizing all statistics in one table
hotels_europe_london_viena_tab <- hotels_europe_london_viena |>
  dplyr::group_by(city) |>
  dplyr::summarise(
    n = length(price), mean = mean(price), median = median(price), min = min(price), max = max(price),
    sd = sd(price), skew = ((mean(price) - median(price)) / sd(price))
  )
hotels_europe_london_viena_tab

# CASE STUDY: Measuring Home Team Advantage in Football
# load data
games <- read_csv("input/epl_games.csv")

# looking at 2016/17 season only
games <- subset(games, season == 2016)

# adding a goal difference variable
games <- games |>
  dplyr::mutate(home_goaladv = goals_home - goals_away)

# summary statistics
summary(games$home_goaladv)
psych::describe(games$home_goaladv)

# plotting the distribution of the goal difference variable
p1 <- ggplot(data = games, aes(x = home_goaladv, y = (..count..) / sum(..count..))) +
  geom_histogram(
    color = color.outline, fill = theme_colors[1],
    size = 0.2, alpha = 0.8, show.legend = F, na.rm = TRUE,
    binwidth = 1
  ) +
  geom_text(stat = "count", aes(label = round((..count..) / sum(..count..) * 100, 1)), hjust = 0.5, vjust = -0.5, size = 2) +
  labs(x = "Goal difference", y = "Share of games (percent)") +
  scale_x_continuous(expand = c(0.05, 0.05), limits = c(-6, 6), breaks = seq(-6, 6, by = 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.25), breaks = seq(0, 0.25, by = 0.05), labels = scales::percent_format(accuracy = 5L)) +
  theme_bg()
p1
```

### Tables

To produce a good table, one needs to to think about its **usage**, choose **encoding** and **scaffolding** accordingly, and may add **annotation**. One important type wants to communicate the main result, or results, of the analysis. A table of this type is called a **result table**, or a communication table. Good communication tables are focused on one message. The other main table type is the **documentation table**. Its aim is to document exploratory data analysis. Documentation tables describe the structure of the data, one or more variables, or some other features such as missing values or extreme values. Such tables are also used to summarize the results of data cleaning and restructuring processes by showing numbers of observations and statistics of important variables for the original data and the data we chose to work with.

Encoding here means what numbers to present, and in what detail. Documentation tables tend to be large and include everything that is, or may become, important. In contrast, communication tables should be simple and focused. A good practice of encoding is to show totals together with components. In a documentation table with numbers of observations in subgroups in the data, the total number of observations is usually included. Another good practice is to include the number of observations in all tables. The second question of encoding is how much detail the numbers should have in the table. Communication tables should have no more detail than necessary. Usually, documentation tables have numbers in more detail.

```{python summary_statistics_table}
# We calculated the mean, the standard deviation of the goal difference and the relative frequency of games

# importing the library
import pandas as pd

# converting R object to a python object
wms = r.games
wms.shape

# calculating the goal difference
wms["home_goaladv"] = wms["goals_home"] - wms["goals_away"]
wms.head()

# making a table to present the results
pd.DataFrame.from_dict(
    {
        "Statistics": [
            "Mean",
            "Standard deviation",
            "Percent positive",
            "Percent zero",
            "Percent negative",
            "Number of observations",
        ],
        "Value": [
            wms["home_goaladv"].describe()["mean"],
            wms["home_goaladv"].describe()["std"],
            (wms["home_goaladv"] > 0).sum() / wms["home_goaladv"].shape[0] * 100,
            (wms["home_goaladv"] == 0).sum() / wms["home_goaladv"].shape[0] * 100,
            (wms["home_goaladv"] < 0).sum() / wms["home_goaladv"].shape[0] * 100,
            wms["home_goaladv"].describe()["count"],
        ],
    }
).round(1)
```

## Theoretical Distributions

Theoretical distributions are distributions of variables with idealized properties. Instead of showing frequencies in data, they show more abstract probabilities: the likelihood of each value (or each interval of values) in a more abstract setting. That more abstract setting is a hypothetical "data" or "population", or the abstract space of the possible realizations of events.

They can be of great help when we want to understand important characteristics of variables in our data. If a variable in our data is well approximated by a theoretical distribution, we can simply attribute those properties to the variable without having to check those properties over and over. Another reason to know about theoretical distributions is that some of them are useful what happens when we want to generalize from the data we have.

The **normal distribution** is a pure theoretical construct in the sense that it was derived mathematically from another distribution, the **binomial**. Variables with a normal distribution can in principle take on any value from negative infinity to positive infinity. The histogram of the normal distribution is bell shaped, and is characterized by two **parameters**, usually denoted as $\mu$ and standard deviation $\sigma$. They refer to the mean ($\mu$) and the standard deviation ($\sigma$). A special case of the normal distribution is the **standard normal distribution**. It is a normal distribution with parameters $\mu = 0$ and $\sigma = 1$. If variable *x* is normally distributed with mean $\mu$ and standard deviation $\sigma$, its transformed version is distributed standard normal if we take out the mean and divide this difference by the standard deviation: $\frac{(x-\mu)}{\sigma}$.

It turns out that when we transform a normally distributed variable by adding or multiplying by a number, the result is another normally distributed variable, with appropriately transformed mean and standard deviation. It also turns out that when we add two normally distributed variables, the resulting variable is also normally distributed, and its mean is the sum of the means of the two original variables.

Some variables in real life are well approximated by the normal distribution. However, the normal is a bad approximation to real-life variables with extreme values. Extreme values are very unlikely in the normal distribution. Besides the absence of extreme values, symmetry is another feature of the normal distribution that makes it a bad approximation to many economic variables. Earnings, income, wealth, and firm productivity are usually asymmetrically distributed with a long right tails. One theoretical distribution that may be a better approximation to such variables is the lognormal distribution.

The **lognormal distribution** is very asymmetric, with a long right tail, potentially including many extreme values at the positive end. The lognormal distribution is derived from the normal distribution. If we take a variable that is distributed normally *(x)* and raise *e* to its power ($e^x$), the resulting variable is distributed lognormally. The old variable is the **natural logarithm** of the new variable. Because we raised *e* to the power of the original variable, the values of the resulting lognormal variable are always positive. They range between zero and positive infinity (never reaching either). By convention, the parameters of the lognormal are the mean $\mu$ and the standard deviation $\sigma$ of the original, normally distributed variable, which is the logarithm of the new variable. There are real-life variables that are approximately lognormally distributed. These include distributions of price, income, and firm size. Variables are well approximated by the lognormal if they are the result of many things multiplied together (the natural log of them is thus a sum).

```{r theoretical_distributions}
# CASE STUDY: Distributions of Body Height and Income
# load data
hrs <- read_csv("input/hrs_height_income.csv")

# parsing height var as numeric
hrs$height <- as.numeric(as.character(hrs$height))

# looking at the overall height distribution
Hmisc::describe(hrs$height)

# filtering height of women age 55-59 
filtered_women_height <- hrs |>
  dplyr::filter(age >= 55 & age < 60 & female == 1 & height > 1.3 & height < 2.1)

Hmisc::describe(filtered_women_height$height)

# plotting women height
p1 <- ggplot(filtered_women_height, aes(x = height)) +
  geom_histogram(aes(y = ..density..),
    binwidth = 0.025, boundary = min(filtered_women_height$height),
    fill = color[1], color = color.outline, alpha = 0.8, closed = "left"
  ) +
  stat_function(
    fun = dnorm, colour = color[2],
    args = with(filtered_women_height, c(mean = mean(height), sd = sd(height)))
  ) +
  scale_y_continuous("Density",
    position = "right", expand = c(0, 0), limits = c(0, 6),
    sec.axis = sec_axis(~ . * 0.025, name = "Percent", breaks = seq(0, 0.15, by = 0.025), labels = percent_format(accuracy = 0.1))
  ) +
  theme_bg() +
  xlab("Height (meters)")
p1

# filtering income of women age 55-69
filtered_women_income <-  hrs  |> 
  dplyr::filter(age >= 55 & age < 60 & female == 1 & hhincome > 1 & hhincome < 1000)

Hmisc::describe(filtered_women_income$hhincome)

# plotting income histogram
p2 <- ggplot(filtered_women_income, aes(x = hhincome)) +
  geom_histogram(aes(y = (..count..) / sum(..count..)),
    binwidth = 20, boundary = 0, closed = "left",
    fill = color[1], color = color.outline, alpha = 0.8
  ) +
  ylab("Percent") +
  xlab("Household income (thousand USD)") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1000), breaks = seq(0, 1000, by = 100)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.05), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()
p2

# log of household income
filtered_women_income <- filtered_women_income |>
  dplyr::mutate(lnincome = log(hhincome))

# plotting log of household income
p3 <- ggplot(filtered_women_income, aes(x = lnincome)) +
  geom_histogram(aes(y = ..density..),
    binwidth = 0.25, boundary = 0, closed = "left",
    fill = color[1], color = color.outline, alpha = 0.8
  ) +
  stat_function(
    fun = dnorm, colour = color[2],
    args = with(filtered_women_income, c(mean = mean(lnincome), sd = sd(lnincome)))
  ) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 8), breaks = seq(0, 8, by = 1)) +
  scale_y_continuous("Density",
    position = "right", expand = c(0, 0), limits = c(0, 0.4),
    sec.axis = sec_axis(~ . * 0.25,
      name = "Percent", breaks = seq(0, 0.1, by = 0.025),
      labels = percent_format(accuracy = 0.1)
    )
  ) +
  theme_bg() +
  ylab("Percent") +
  xlab("ln(household income, thousand USD)")
p3
```

# CHAPTER 04: Comparison and Correlation

Much of data analysis is built on comparing values of a *y* variable by values of an *x* variable, or more *x* variables. Such a comparison can uncover the **patterns of association** between the two variables: whether and how observations with particular values of one variable *(x)* tend to have particular values of the other variable *(y)*.

The role of *y* is different from the role of *x*. It's the values of *y* we are interested in, and we compare observations that are different in their *x* values. This asymmetry comes from the goal of our analysis. One goal of data analysis is predicting the value of a *y* variable with the help of other variables. The prediction itself takes place when we know the values of those other variables but not the *y* variable. To predict *y* based on the other variables we need a rule that tells us what the predicted *y* value is as a function of the values of the other variables. Such a rule can be devised by analyzing data where we know the *y* values, too.

The other most frequent goal of data analysis is to learn about the effect of a **causal variable** *x* on an **outcome variable** *y*. Here, we typically are interested in what the value of *y* would be if we could change *x*.

In sum, deciding on what's *y* and what's *x* in the data is the first step before doing any meaningful analysis.

-   For prediction we want to know what value of *y* to expect for different values of various *x* variables, such $x_1$, $x_2$,...
-   For causal analysis we want to know what value of *y* to expect if we changed the value of *x*, often comparing observations that are similar in other variables $(z_1)$, $(z_2)$, ...

```{r patterns_of_association}
# CASE STUDY: Management quality and firm size: describing patterns of association
# load data
wms <- read_csv("input/wms_da_textbook.csv")

# sample selection
wms <- wms |> 
  dplyr::filter(country=="Mexico" & wave==2013 & emp_firm>=100  & emp_firm<=5000)

# summary
summary(wms$emp_firm)
Hmisc::describe(wms$emp_firm)

wms  |> 
  dplyr::select(management, emp_firm)  |> 
  dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))

# plotting distribution of the management score
p1 <- ggplot(data = wms, aes(x = management)) +
  geom_histogram_da(binwidth = 0.25, type = "percent", boundary = 0) +
  labs(x = "Management score", y = "Percent") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(1, 5)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.25), breaks = seq(0, 0.25, by = 0.05), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()
p1

# plotting the distribution of employees
p2 <- ggplot(data = wms, aes(x = emp_firm)) +
  geom_histogram_da(binwidth = 200, type = "percent") +
  labs(x = "Firm size (employment)", y = "Percent") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.05), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()
p2

# creating variable log
wms$lnemp = log(wms$emp_firm)
Hmisc::describe(wms$lnemp)

# plotting the natural log of number of employees
p3 <- ggplot(data = wms, aes(x = lnemp)) +
  geom_histogram_da(binwidth = 0.25, type = "percent", boundary = 0) +
  labs(x = "Firm size (ln(employment))", y = "Percent") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(4, 9)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.2), breaks = seq(0, 0.2, by = 0.04), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()
p3
```

## Conditioning

The word statisticians use for comparison is **conditioning**. When we compare the values of *y* by the values of *x*, we condition *y* on *x*. *y* is also called the **outcome variable**; *x* is also called the **conditioning (or predictor) variable**.

When data analysts want to uncover values of *y* for observations that are different in *x* but similar in *z*, they do one more step of conditioning: they compare *y* by *x* conditional on *z*. That is called **further conditioning** or a **conditional comparison**. In practice we explore conditional probabilities, conditional means, and conditional distributions.

### Conditional Probabilities

The probability of a value of a variable in a data set is its relative frequency (percentage). **Conditional probability** is the probability of one event if another event happens. The event the conditional probability is about is called the **conditional event**; the other event is called the **conditioning event**. Conditional probabilities are denoted as $P(event_1\ |\ event_2)$: the probability of $event_1$ conditional on $event_2$.

Joint probabilities are related to conditional probabilities. The **joint probability** of two events is the probability that both occur: $P(event_1\ \&\ event_2)$. When two events are mutually exclusive, their joint probability is zero (the two never happen together).

Another probability related to two events denotes the likelihood that one event or the other happens. This is the sum of the two probabilities minus their joint probability: $P(event_1\ OR\  event_2) = P(event_1) + P(event_2) - P(event_1\ \&\ event_2)$. If the two events are mutually exclusive, we subtract zero from the sum of the two probabilities.

The conditional probability can be expressed as the corresponding joint probability divided by the probability of the conditioning event: $$P(event_1\ |\ event_2) = \frac{P(event_1\ \&\ event_2)}{P(event_2)}$$ Two events are **independent** if the probability of one of the events is the same regardless of whether or not the other event occurs. In the language of conditional probabilities this means that the conditional probabilities are the same as the unconditional probabilities: $P(event_1\ |\ event_2) = P(event_1)$ and $P(event_2\ |\ event_1) = P(event_2)$. Less intuitive, but also true is that the joint probability of independent events equals the product of their individual probabilities: $P(event_1\ \&\ event_2) = P(event_1)\ X\ P(event_2)$.

In data, the events refer to values of variables. Often, the conditional variable, *y*, is a binary variable: *y = 0* or *y = 1*. Then the conditional probability is the probability that *y = 1* if *x* has some value: $P(y = 1|x = value)$. Since *y* is binary, we know the probability of *y = 0* if we know the probability of *y = 1*, be it a conditional or unconditional probability. When *x* is binary too, there are two conditional probabilities: $$P(y = 1\ |\ x = 1)$$ $$P(y = 1\ |\ x = 0)$$ With more values for either of the two variables *(y, x)*, we have more numbers to compare: $P(y = value\ |\ x = value)$. With relatively few values, visualization often helps. To visualize conditional probabilities if both *y* and *x* have few values, a good visualization is to show stacked bar charts of *y* for the values of *x*.

```{r conditional_probability}
# CASE STUDY: Management quality and firm size: describing patterns of association
# generating employment bins
wms$emp3bins <- ifelse(wms$emp_firm < 200, 1,
  ifelse(wms$emp_firm >= 200 & wms$emp_firm < 1000, 2,
    ifelse(wms$emp_firm >= 1000, 3, 100)
  )
)

Hmisc::describe(wms$emp3bins)

# converting created variable as factor
wms$emp3bins <- as.factor(wms$emp3bins)

# creating new wms for lean management
wms_lean <- wms |>
  dplyr::select(emp3bins, lean1) |>
  dplyr::group_by(emp3bins, lean1) |>
  dplyr::summarise(Count = n()) |>
  dplyr::mutate(Percent = round(Count / sum(Count), digits = 5)) |>
  dplyr::ungroup() # use |> ungroup() when do multiple times group_by

# plotting lean management
p1 <- ggplot(data = wms_lean, aes(x = emp3bins, y = Percent, fill = factor(lean1, levels = rev(unique(lean1))))) +
  geom_bar(stat = "identity", position = "fill", width = 0.6, color = "white", size = 0.5, alpha = 0.8) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = seq(0, 1, by = 0.2), labels = scales::percent_format()) +
  scale_x_discrete(labels = c("1" = "Small", "2" = "Medium", "3" = "Large")) +
  scale_fill_manual(values = c(color[3], color[1], color[5], color[2], color[4]), name = NULL) +
  labs(x = "Firm size (employment), 3 bins", y = "Percent", title = "Lean Management") +
  theme_bg() +
  theme(legend.position = "right")
p1

# creating new wms for performance tracking
wms_per <- wms |>
  dplyr::select(emp3bins, perf2) |>
  dplyr::group_by(emp3bins, perf2) |>
  dplyr::summarise(Count = n()) |>
  dplyr::mutate(Percent = round(Count / sum(Count), digits = 5)) |>
  dplyr::ungroup()

# plotting performance tracking
p2 <- ggplot(data = wms_per, aes(x = emp3bins, y = Percent, fill = factor(perf2, levels = rev(unique(perf2))))) +
  geom_bar(stat = "identity", position = "fill", width = 0.6, color = "white", size = 0.5, alpha = 0.8) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1), breaks = seq(0, 1, by = 0.2), labels = scales::percent_format()) +
  scale_x_discrete(labels = c("1" = "Small", "2" = "Medium", "3" = "Large")) +
  scale_fill_manual(values = c(color[3], color[1], color[5], color[2], color[4]), name = NULL) +
  labs(x = "Firm size (employment), 3 bins", y = "Percent", title = "Performance Tracking") +
  theme_bg() +
  theme(legend.position = "right")
p2
```

### Conditional Distribution and Conditional Expectation

Just as all variables have a distribution, all *y* variables have a **conditional distribution** if conditioned on an *x* variable. The simplest case is a binary *x* when the conditional distribution of *y* is two distributions, one for each of the two *x* values.

Conditional distributions with few *x* values are best presented by visualization: for each *x* value we show stacked bar charts if *y* has few values or histograms if *y* is a quantitative variable with many values. Make sure that the histograms are fully comparable across the *x* values: most importantly, they have the same bins of *y*, and they have the same scale on the vertical axis. Comparing histograms can reveal qualitative differences in the distributions. To make quantitative comparisons, however, we need to compare statistics that summarize the important features of distributions.

The most important conditional statistics is the **conditional mean**, also known as the **conditional expectation**, which shows the mean of *y* for each value of *x*: $E[y|x]$. Note that while the overall mean of *y*, $E[y]$, is a single number in a data table, the conditional mean, $E[y|x]$, varies in the data, because it can be different values of *x*.

Analogously, we can look at conditional medians, standard deviations, and so on. Comparing box plots and violin plots is a great way to visualize conditional statistics of *y* when *x* has few values.

### Conditional Distribution and Conditional Expectation with Quantitative *x*

When the *x* is quantitative with many values, it is usually impossible or impractical to plot histograms or compute the conditional mean for each value of *x*. First, there are too many values of *x* and, typically, too few observations for each value. Second, even if we had enough observations, the resulting statistics or pictures would typically be too complex to make sense of. We have two approaches to deal with these problems. The first approach circumvents the problem of too many *x* values by reducing them through creating bins. Creating bins from *x* is not only the simplest approach, but it often produces powerful results. Usually with just three bins of *x* we can capture the most important patterns of association between a quantitative *y* and a quantitative *x*.

Visualization of conditional means of *y* for bins of *x* is called a **bin scatter**. A bin scatter is a figure with the values of the binned *x* on the horizontal axis and the corresponding conditional mean values of the binned *y* on the vertical axis. It is a good practice to visualize bin scatters with meaningful *x* values for the *x* bins. The second approach keeps *x* as it is. The most widely used tool is the **scatterplot**, which is a visualization of the **joint distribution** of two variables. The joint distribution of two variables is the frequency of each value combination of the two variables.

```{r conditional_distributions}
# CASE STUDY: Management quality and firm size: describing patterns of association
# re-coding employee bins
wms$emp3bins <- ifelse(wms$emp3bins == 1, 150,
  ifelse(wms$emp3bins == 2, 600,
    ifelse(wms$emp3bins == 3, 3000, NA)
  )
)

# summary
wms  |> 
  dplyr::select(emp_firm, emp3bins)  |> 
  dplyr::group_by(emp3bins)  |> 
  dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))

# generating variables by mean (3 bins)
wms_mean03 <- wms  |> 
  dplyr::group_by(emp3bins)  |> 
  dplyr::summarize(management_emp3bins = mean(management))

# plotting conditional mean (3 bins of employment)
p1 <- ggplot(data = wms_mean03, aes(x = emp3bins, y = management_emp3bins)) +
  geom_point(size = 2, color = color[3], fill = color[1], shape = 21, alpha = 0.8, na.rm = T) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(2.4, 3.4), breaks = seq(2.4, 3.4, by = 0.2)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  labs(x = "Firm size (employment), 3 bins", y = "Management score") +
  theme_bg()
p1

# generating employment bins of 10
wms$emp10bins <- wms$emp_firm  |>  ggplot2::cut_number(10)

# summary
wms_summary <- wms  |> 
  dplyr::select(emp_firm, emp10bins)  |> 
  dplyr::group_by(emp10bins)  |> 
  dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))
wms_summary

# re-coding
levels(wms$emp10bins) <- wms_summary  |> 
  dplyr::pull(mean)  |> 
  round()

wms$emp10bins <- as.numeric(levels(wms$emp10bins))[wms$emp10bins]

# summary
wms  |> 
  dplyr::select(emp_firm, emp10bins)  |> 
  dplyr::group_by(emp10bins)  |> 
  dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))

# generating variables by mean (10 bins)
wms_mean10 <- wms  |> 
  dplyr::group_by(emp10bins)  |> 
  dplyr::summarize(management_emp10bins = mean(management))

# plotting conditional mean (10 bins of employment)
p2 <- ggplot(data = wms_mean10, aes(x = emp10bins, y = management_emp10bins)) +
  geom_point(size = 2, color = color[3], fill = color[1], shape = 21, alpha = 0.8, na.rm = T) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(2.5, 3.5), breaks = seq(2.5, 3.5, by = 0.25)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 3500), breaks = seq(0, 3500, by = 500)) +
  labs(x = "Firm size (employment), 10 bins", y = "Management score") +
  theme_bg()
p2

# this is a simpler solution, similar looking graph (not in book):
binsreg::binsreg(wms$management, wms$emp_firm, nbins = 3)
binsreg::binsreg(wms$management, wms$emp_firm, nbins = 10)

# plotting avg score by employment
p3 <- ggplot(data = wms, aes(x = emp_firm, y = management)) +
  geom_point(color = color[1], size = 1.5, shape = 16, alpha = 0.8, show.legend = FALSE, na.rm = TRUE) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
  labs(x = "Firm size (employment)", y = "Management score") +
  theme_bg()
p3

# creating log var
wms$lnemp <-  log(wms$emp_firm)

# plotting avg score by employment (log)
p4 <- ggplot(data = wms, aes(x = lnemp, y = management)) +
  geom_point(color = color[1], size = 1.5, shape = 16, alpha = 0.8, show.legend = FALSE, na.rm = TRUE) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(4, 9), breaks = seq(4, 9, by = 1)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
  labs(x = "Firm size (ln(employment))", y = "Management score") +
  theme_bg()
p4

# transforming as bin categories factor
wms$emp3bins <- as.factor(wms$emp3bins)
levels(wms$emp3bins) <- c("Small", "Medium", "Large")

# plotting boxplots
p5 <- ggplot(data = wms, aes(x = emp3bins, y = management)) +
  stat_boxplot(aes(group = emp3bins), geom = "errorbar", width = 0.5, color = c(color[2], color[1], color[3]), size = 0.5, na.rm = T) +
  geom_boxplot(aes(group = emp3bins), color = c(color[2], color[1], color[3]), fill = c(color[2], color[1], color[3]), size = 0.5, width = 0.5, alpha = 0.3, na.rm = T) +
  labs(x = "Firm size (employment), 3 bins", y = "Management score") +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
  theme_bg()
p5

# plotting violin plots
p6 <- ggplot(data = wms, aes(x = emp3bins, y = management, color = emp3bins, fill = emp3bins)) +
  geom_violin(aes(group = emp3bins), size = 0.3, alpha = 0.3, trim = F, show.legend = F, na.rm = TRUE) +
  geom_boxplot(aes(group = emp3bins), color = c(color[2], color[1], color[3]), fill = c(color[2], color[1], color[3]), size = 0.5, width = 0.2, alpha = 0.3, na.rm = T) +
  labs(x = "Firm size (employment), 3 bins", y = "Management score") +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 6), breaks = seq(0, 6, 1)) +
  scale_color_manual(
    name = "",
    values = c(color[2], color[1], color[3])
  ) +
  scale_fill_manual(
    name = "",
    values = c(color[2], color[1], color[3])
  ) +
  theme_bg()
p6
```

### Dependence, Covariance and Correlation

**Dependence of two variables**, also called **statistical dependence** means that the conditional distributions of one variable *(y)* are not the same when conditional on different values of the other variable *(x)*. In contrast, **independence of variables** means that the distribution of one conditional on the other is the same, regardless of the value of the conditioning variable. Dependence of *y* and *x* may take many forms, but the most important form of dependence is **mean-dependence**: the mean of *y* is different when the value of *x* is different.

The **covariance** and the **correlation coefficient** are measures of this mean-dependence. The two measures are very closely related: the correlation coefficient is the standardized version of the covariance.

The formula for the covariance between two variables *x* and *y* in a dataset with *n* observations is: $$Cov[x, y] = \frac{\sum_i(x_i -\hat x)(y_i - \hat y)}{n}$$ The correlation coefficient divides this by the product of the two standard deviations: $$Corr[x, y] = \frac{Cov[x, y]}{Std[x]Std[y]}$$ The covariance may be any positive or negative number, while the correlation coefficient is bound to be between -1 and 1. But their sign is always the same: the covariance is zero when the correlation coefficient is zero **(uncorrelated)**; the covariance is positive when the correlation coefficient is positive **(positively correlated)**; the covariance is negative when the correlation coefficient is negative **(negative correlated)**. The magnitude of the correlation coefficient shows the strength of the association: a correlation of 0.2 or less can be considered weak, and a correlation above 0.7 is usually considered strong. If two variables are independent, the covariance and the correlation coefficient are both zero.

### From Laten Variables to Observed Variables

Often, the *y* or *x* variables we have in question are abstract concepts: the quality of management of a firm, skills of an employee, health of a person...Typically, such variables are not parts of an actual dataset, and they can't be because they are too abstract. Such variables are called **latent variables**. Data analysis can help answer questions involving latent variables only by substituting observed variables for them. Those observed variables are called **proxy variables**.

Data analysis compares values of measured variables. Even if those variables are supposed to measure abstract concepts, it's never the abstract concepts themselves that we have in our data. A specific issue arises when our data contains not one but more variables that could serve as proxies to the latent variable we want to examine. The question here is how to combine multiple observed variables:

-   Use one of the observed variables
-   Take the average (or sum) of the observed variables
-   Use principal component analysis (PCA) to combine the observed variables

Using one measured variable and excluding the rest has the advantage of easy interpretation, and the disadvantage of discarding potentially useful information contained in the other variables.

Taking the average of all measured variables makes use of all information in a simple way. When all of those variables are measured using the same scale, this approach yields a combined measure with a natural interpretation. When the variables are measured at different scales, we need to bring the observed variables to the same scale, usually by standardizing them **(z-score)**. By taking a simple average of all these variables we give them equal weight. This may be a disadvantage if some of the variables are better measures of the latent variable than others.

**Principal component analysis (PCA)** is a method to give higher weights to the observable variables that are better measures. PCA finds those weights by examining how strongly they would be related with the weighted average.

```{r correlation}
# CASE STUDY: Management quality and firm size: describing patterns of association
# calculating overall correlation coefficient
cor(wms$management, wms$emp_firm, use = "complete.obs")

# looking at the most recent industry code for the firms
table(wms$sic)

# recoding industry codes
wms$industry_broad[wms$sic <= 21] <- "food_drinks_tobacco"
wms$industry_broad[wms$sic >= 22 & wms$sic <= 23 | wms$sic == 31] <- "textile_apparel_leather_etc"
wms$industry_broad[wms$sic >= 24 & wms$sic <= 27] <- "wood_furniture_paper"
wms$industry_broad[wms$sic >= 28 & wms$sic <= 30] <- "chemicals_etc"
wms$industry_broad[wms$sic >= 32 & wms$sic < 35] <- "materials_metals"
wms$industry_broad[wms$sic >= 35 & wms$sic < 37] <- "electronics"
wms$industry_broad[wms$sic == 37] <- "auto"
wms$industry_broad[wms$sic >= 38] <- "other"

table(wms$industry_broad)

# summary management
wms  |> 
  dplyr::select(management, industry_broad)  |> 
  dplyr::filter(!is.na(industry_broad))  |> 
  dplyr::group_by(industry_broad)  |> 
  dplyr::summarise(
    Min = min(management),
    Max = max(management),
    SD = sd(management),
    Median = median(management),
    n()
  )

# summary employment
wms  |> 
  dplyr::select(emp_firm, industry_broad)  |> 
  dplyr::filter(!is.na(industry_broad))  |> 
  dplyr::group_by(industry_broad)  |> 
  dplyr::summarise(
    Min = min(emp_firm),
    Max = max(emp_firm),
    SD = sd(emp_firm),
    Median = median(emp_firm),
    n()
  )

# calculating correlation per industry broad
cor <- wms  |> 
  dplyr::group_by(industry_broad)  |> 
  dplyr::summarize(COR = cor(management, emp_firm))

# creating a table with the average of all relevant variables for the latent
wms_tab <- wms  |> 
  dplyr::select(emp_firm, industry_broad, management)  |> 
  dplyr::group_by(industry_broad)  |> 
  dplyr::summarise(Mean = mean(management), Obs = n())

# adding correlation to the table
wms_tab$cor <- cor$COR

# recoding variables
wms_tab$industry_broad <- wms_tab$industry_broad  |>  dplyr::recode(
  auto = "Auto",
  chemicals_etc = "Chemicals",
  electronics = "Machinery, equipment, electronics",
  food_drinks_tobacco = "Food, drinks, tobacco",
  materials_metals = "Materials, metals",
  textile_apparel_leather_etc = "Textile, apparel, leather",
  wood_furniture_paper = "Wood, furniture, paper",
  other = "Other"
)

# adding the last row with all category
last_row <- wms_tab  |>  dplyr::summarise(Mean = mean(Mean), Obs = sum(Obs), cor = mean(cor))
last_row$industry_broad <- "All"

wms_tab <- wms_tab  |>  dplyr::add_row(
  industry_broad = last_row$industry_broad,
  Mean = last_row$Mean,
  cor = last_row$cor,
  Obs = last_row$Obs
)

# formatting the table
wms_tab <- wms_tab  |>  dplyr::select(industry_broad, cor, Mean, Obs)
wms_tab
```

# CHAPTER 05: Generalization from Data (inference)

The act of generalization is called **inference**: we infer something from our data about a more general phenomenon because we want to use that knowledge in some other situation. Inference is best broken down into two steps: _generalizing from the data to the general pattern it represents_, and _generalizing from that general pattern to the general pattern that is behind the situation we truly care about_.

The first step is **statistical inference**, which aims at generalizing to the situation that our data represents, using statistical methods. The simplest case is a random sample that represents a well-defined **population**. The **general pattern** is a more abstract concept. For our data to represent this general pattern, it is not necessary that all variables have the same distribution. Instead, we require that the specific pattern is the same. Generalizing to a general pattern that our data represents can make sense even when our data covers an entire population.

The second step is assessing the __external validity__ of the results. External validity is about whether the general pattern our data represents is the same as the general pattern that would be relevant for the situation we truly care about. If the general pattern behind the situation we care about and the data we have is the same, what we uncover from our data has high external validity for the situation we care about. If, on the other hand, the general pattern behind the situation we care about is different from the general pattern that our data represents, whatever we uncover from our data would have low external validity. External validity is best thought of as a continuum, from very high (our data represents the same general pattern we are interested in) through medium (the two general patterns are similar but not the same) to very low (the two general patterns are very different).

Assessing external validity cannot be done with statistical methods. In practice, the goal of most analyses is a quantitative summary of patterns: a __statistic__. We want to infer the __true value__ of the statistic after having computed its __estimated value__ from the actual data. The true value is the value in a population or a general pattern. Thus, external validity is about the extent to which the true value of the statistic is similar in the general pattern our data represents and the general pattern that's behind the situation we care about. The goal of statistical inference is to uncover the true value of the statistic in the general pattern, or population, that our data represents. The starting point of statistical inference is the estimated value of the statistic that we calculate from our data. It is called an estimated value because the reason for calculating it is to uncover (estimate) the true value.

```{r inference}
# CASE STUDY: What likelihood of loss to expect on a stock portfolio
# load data
sp500 <- read_csv("input/SP500_2006_16_data.csv", na = c("", "#N/A"))
# subsetting all the values that are NOT NAs
sp500 <- subset(sp500, VALUE != "NA")

# creating percent return
sp500 <- sp500 |> 
  dplyr::mutate(pct_return = (VALUE - dplyr::lag(VALUE)) / dplyr::lag(VALUE) * 100)

# creating date variable
sp500$year <- format(sp500$DATE, "%Y")
sp500$month <- format(sp500$DATE, "%m")
sp500$year <- as.numeric(sp500$year)
sp500$month <- as.numeric(sp500$month)
sp500$yearmonth <- sp500$year * 100 + sp500$month

# plotting daily returns
p1 <- ggplot(sp500, aes(pct_return)) +
  geom_histogram_da(binwidth = 0.25, type = "frequency") +
  geom_vline(xintercept = -5, size = 0.7, color = color[2]) +
  labs(x = "Daily return (percent)", y = "Frequency") +
  coord_cartesian(xlim = c(-10, 10), ylim = c(0, 400)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_segment(aes(x = -6, y = 220, xend = -5, yend = 220), arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = -8, y = 220, label = "5% loss", size = 2.5) +
  theme_bg()
p1
```

## Repeated Samples, Sampling Distribution and Standard Error (SE)

The conceptual background to statistical inference is __repeated samples__. The basic idea is that the data we observe is one example of many data sets that could have been observed. Each of those potentially observed data sets can be viewed as a sample drawn from the population, or the more abstract general pattern.

The goal of statistical inference is learning the value of a statistic in the population, or general pattern, represented by our data. With repeated samples, the statistic has a distribution: its value may differ from sample to sample. Within each sample the calculated value of statistic is called an __estimate__ of the statistic. The distribution of the statistic is called its __sampling distribution__. The sampling distribution of a statistic shows the values that the estimated statistic takes across repeated samples. The most important aspect of this sampling distribution is its standard deviation: how much spread there is across repeated samples. The standard deviation of the sampling distribution has a specific name: the __standard error (SE)__ of the statistic ($se(\bar X) = \frac{\sigma}{\sqrt N}$)

The sampling distribution has three important properties:

1. The average of the values in repeated samples is approximately equal to its true value (the value of the entire population, or general pattern, represented by our data.) -> __unbiasedness__
2. The sampling distribution is approximately normal. -> __approximate normality__
3. The standard error is smaller the larger the sample, with a proportionally factor of the square root of the sample size. -> __root-n convergence__

Because the sampling distribution is approximately normal, we know that the measured values fall within plus or minus two standard errors (SE) of the truth approximately 95% of the time. Similarly, we know that they fall within plus or minus 1.6 SE of the truth with a 90% chance, and within 2.6 SE with a 99% chance.

```{r standard_error}
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
# Create 10 000 samples, with 500 and 1000 observations in each sample, taken from sp500
# remove first row as it has NA in pct_return
pct_return <- sp500  |> 
  dplyr::filter(!is.na(pct_return))  |> 
  dplyr::pull(pct_return)

# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses 
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use. 
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
  samples_pcloss <- c()
  for (i in 1:n_samples) {
    single_sample <- sample(vector, n_obs, replace = FALSE)
    samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
  }
  samples_pcloss
}

set.seed(123)

# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)

# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)

# calculating the se
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))

# calculating CI
left <- mean(nobs_df$nobs_1000) - se
right <- mean(nobs_df$nobs_1000) + se

# plotting simulated number of days with big losses
options(digits = 2)

p2 <- ggplot(nobs_df, aes(nobs_1000)) +
  geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
  labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
  geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
  coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
  geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
  theme_bg()
p2

# comparing density for both simulations
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
  stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
  stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
  labs(x = "Percent of days with losses over 5%", y = "Density") +
  geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
  geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
  geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
  scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
  theme_bg() +
  theme(legend.position = "none")
p3

# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
  geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
  geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
  ylab("Percent") +
  xlab("Percent of days with losses over 5%") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
  scale_color_manual(name = "", values = c(color[2], color[1])) +
  scale_fill_manual(name = "", values = c(color[2], color[1])) +
  theme_bg() +
  theme(
    legend.position = c(0.7, 0.9),
    legend.key.size = unit(x = 0.4, units = "cm"),
    legend.direction = "horizontal"
  )
p4

# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
```

## The Confidence Interval

The __confidence interval (CI)__ is the most important measure of statistical inference. The idea of the CI is to give us a range, in which we can expect the true value to lie with a certain probability. That probability is something data analysts need to pick. Typical probability picks are 95% or 90%.

The confidence interval is constructed as a symmetric range around the estimated value of the statistic in our data. We first calculate the estimated value of the statistic in our data. Then we specify the length of the CI on the two sides of the estimated value. We use the SE to know the length of the appropriate CI. The SE is the general unit of measurement when constructing confidence intervals. The appropriate 95% CI is the $\pm2$ SE or, more precisely, 1.96 SE interval around the estimate from the data. The appropriate 90% CI is the $\pm1.6$ SE interval around the estimate from the data; and the appropriate 99% CI is the $\pm2.6$ SE interval around it.

## Estimating the Standard Error with the Bootstrap Method

The __bootstrap__ is a method that creates the sampling distribution across repeated samples that are representative of the same population, or general pattern, as the original data and also have the same size. The bootstrap method takes the original data and draws many repeated samples of the size of that data. The trick is that the samples are drawn with replacement. The observations are drawn randomly one by one from the original data; once an observation is drawn it is "replaced" into the pool so that it can be drawn again, with the same probability as any other observation. The drawing stops when the sample reaches the size of the original data. The result is a sample of the same size as the original data, yielding a single __bootstrap sample__. This bootstrap sample includes some of the original observations multiple times, and it does not include some of the other original observations. The distribution of a statistic across these repeated bootstrap samples is a good approximation to the sampling distribution that would look like across samples similar to the original data.

![Bootstrap samples](input/bootstrap.png)
The bootstrap method works if the variable we consider is independent across observations. By sampling observations one by one and then replacing them into the pool to be possibly drawn again, we destroy the before-after relationships in the original data. That is not a problem if there is no relationship across observations, which is true in most cross-sectional data.

Since the distribution of a statistic across bootstrap samples is a good approximation of the sampling distribution, it gives a good approximation of the standard error, too. The __bootstrap estimate of the standard error__ is the standard deviation of the estimated values of the statistic across the bootstrap samples. Once we have the standard error, we have everything we need for constructing the confidence interval of our choice.

The bootstrap method have its limitations, on top of the independence, it tends to provide very poor standard error estimates in the presence of extreme values. Moreover, it may produce poor standard error estimates in small samples if the statistic is not the mean but the median or some other quantile.

```{r bootstrap-se}
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
set.seed(573164)

M <- 10000

Results <- matrix(rep(0, (M * 10)), nrow = M, ncol = 10)

for (i in 1:M) {
  bsample <- sample(sp500$pct_return, size = dim(sp500)[1], replace = TRUE)

  for (j in 1:10) {
    loss <- as.numeric(bsample < (-j)) * 100
    Results[i, j] <- mean(loss, na.rm = T)
  }
}

Results <- as_tibble(Results)
names(Results) <- c(
  "loss1", "loss2", "loss3", "loss4", "loss5", "loss6",
  "loss7", "loss8", "loss9", "loss10"
)

p1 <- ggplot(Results, aes(loss5)) +
  geom_histogram_da(type = "frequency", binwidth = 0.04, boundary = 0, closed = "left") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1200), breaks = seq(0, 1200, 200)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.2), breaks = seq(0, 1.2, 0.1)) +
  labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
  theme_bg()
p1
```
## The Standard Error Formula

There is a relatively simple formula for the standard error (SE) of the average ($\bar x$). The formula includes the root-n factor and just one another element. $\bar x$ is the estimate of the true mean value of _x_ in the general pattern (or the population).

The __standard error formula__ for $\bar x$ is $$SE(\bar x) = \frac{1}{\sqrt n} Std[x]$$ where $Std[x]$ is the standard deviation of the variable in the data, and _n_ is the number of observations in the data. The standard error is larger, the smaller the sample and the larger the standard deviation of the variable.

```{r se-formula}
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
# using the psych package to figure out the sd = 0.07
psych::describe(sp500$pct_return > -5)

# calculating the denominator of the SEM formula = 0.02
x <- sqrt((1 / 2519))

# calculating the product of both operations -> 0.02 * 0.07 = 0.0014 (0.14%) or the standard error of the mean
sem <- x * 0.07
sem

# calculating the 95% CI
right <- 0.5 + 2 * 0.14
left <- 0.5 - 2 * 0.14
c(left, right)
```

## External Validity

The confidence interval summarizes uncertainty about the true value of the statistics in the population, or the general pattern, that our data represents. An important part in inference is defining the population, or general pattern, we care about (the future), and assessing how close it is to the population, or general pattern, our data represents (the past). __External validity__ is the concept that captures the similarity of those two general patterns. 

With _high_ external validity, the confidence interval captures all uncertainty about our estimate. With _low_ external validity, it does not. To assess external validity we need substantive knowledge of the situation. The most important challenges to external validity may be collected in three groups: __time__, __space__, and __sub-groups__. 

First, _stability in time_. The data we analyze is always from the past, whereas the situation we care about is almost always in the future. Thus, in order to generalize from the data, we need to assume that the general pattern that was relevant in the past will remain relevant for the future. A specific issue with time is the occurrence of extreme values. Our data may be free of extreme values because some very rare extreme events may not have happened in the historic period that our data represents. However, this does not imply that such a rare extreme event could not happen in the future we care about.

Second, _stability through space_. The data we analyze may come from a different country, region, or city than the situation we care about.

Third, _stability across subgroups_. A pattern may be strong among one group of people, but it may not be there for people in different professional, age, gender, cultural, or income groups.

# CHAPTER 06: Testing Hypotheses

A hypothesis is a statement about a population, or general pattern. Testing a hypothesis amounts to gathering information from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population, or general pattern. Thus, __hypothesis testing__ means analyzing the data at hand to make a decision about the hypothesis. Two decisions are possible: rejecting the hypothesis or not rejecting it. We reject the hypothesis if there is enough evidence against it. We don't reject it if there isn't enough evidence against it. We may have insufficient evidence against a hypothesis either if the hypothesis is true or if it is not true but the evidence is weak.

Testing a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity. The first step is defining the statistic that would answer our question. Often that statistic is an _average_, or the _difference between two average_ values.

```{r dist-online-offline}
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
pd <- read.csv("input/online_offline_ALL_clean.csv")

# filter the data
pd <- pd |>
  filter(COUNTRY == "USA") |>
  filter(PRICETYPE == "Regular Price") |>
  filter(is.na(sale_online)) |>
  filter(!is.na(price)) |>
  filter(!is.na(price_online))

# drop errors (+1000 dollars)
pd <- pd |> filter(price < 1000)

# creating diff variable
pd <- pd |> mutate(diff = price_online - price)

# Check the main descriptives
modelsummary::datasummary(diff ~ Mean + SD + Min + Max + Median + Max, data = pd)

# plotting online-offline price differences
p1 <- ggplot(data = pd, aes(diff)) +
  geom_histogram(
    binwidth = 5, boundary = 0, closed = "left",
    fill = color[1], size = 0.25, alpha = 0.8, show.legend = F, na.rm = TRUE
  ) +
  labs(x = "Online - offline price difference (US dollars)", y = "Frequency") +
  theme_bg() +
  scale_x_continuous(limits = c(-420, 420), breaks = seq(-400, 420, by = 100)) +
  scale_y_continuous(limits = c(0, 6000), breaks = seq(0, 6000, by = 1000), expand = c(0.01, 0.01)) +
  geom_segment(aes(x = 300, y = 500, xend = 415, yend = 20), arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = 300, y = 700, label = "max value= 415", size = 2.5) +
  geom_segment(aes(x = -280, y = 500, xend = -380, yend = 20), arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = -300, y = 700, label = "min value= -380", size = 2.5)
p1

# 4.99999 not 5 -- needed because of data imported from stata may be stored wierdly. 
# pd1 <- subset(pd, abs(pd$diff) < 4.999999)
# Hmisc::describe(pd1$diff)

# plotting online-offline price differences (+-5 dollar price difference) 
p2 <- ggplot(data = pd, aes(diff)) +
  geom_histogram(
    binwidth = 0.5, boundary = -0, closed = "left",
    color = color.outline, fill = color[1], size = 0.25, alpha = 0.8, show.legend = F, na.rm = TRUE
  ) +
  labs(x = "Online - offline price difference (US dollars)", y = "Frequency") +
  theme_bg() +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(limits = c(-5, 5), breaks = seq(-5, 5, by = 1)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 5000), breaks = seq(0, 5000, by = 1000))
p2

p1 | p2   
```
## Null Hypothesis, Alternative Hypothesis

After specifying the statistic that helps answer our question, we need to state two competing hypotheses, of which only one can be true. The first one is the __null hypothesis__ ($H_0$), the second one is the __alternative hypothesis__ ($H_1$). These hypotheses are formulated in terms of the unknown __true value__ of the statistics. Together, the null and the alternative should cover all possibilities we are interested in. $$H_0: s_{true}  = 0 \\ H_1: s_{true} \not =  0$$

The case when we test if $H_1 : s_{true} \not = 0$ is called __two-sided alternative__ as it allows for $s_{true}$ to be either greater than zero or less than zero. The other case is working with a __one-sided alternative__, when we are indeed interested if a statistic is positive. $$H_0 : s_{true} \le 0 \\ H_1 : s_{true} \gt 0$$

## The t-Test

Following the logic of hypothesis testing, when we look at the evidence to see if we want to reject the null or maintain our assumption that it's true, the evidence we look for is how far the estimated value of the statistic is from zero, its hypothesized value. We reject the null if the estimate is large, and we do not reject it if the estimate is not very far. How far is far enough for rejecting requires a measure of the distance -> the __t-statistic__. More generally, the t-statistic is a __test statistic__. __A test statistic is a measure of the distance of the estimated value of the statistic from what its true value would be if $H_0$ were true__.

Consider $H_0: s_{true} = 0$ , $H_1: s_{true} \not =  0$, where $s$ is an average, or the difference of that average from a number, or a difference of two averages. The null and alternative are about the true value of the statistic: $s_{true}$. Our data contains information to compute the estimated value of the statistic: $\hat{s}$. The t-statistic for this hypotheses uses the estimated value $\hat{s}$ and its standard error: $$t = \frac{\hat{s}}{SE(\hat{s})}$$ When $\hat{s}$ is the average of a variable _x_, the t-statistic is simply $$t = \frac{\bar{x}}{SE(\bar{x})}$$ When $\hat{s}$ is the average of a variable _x_ minus a number, the t-statistic is $$t = \frac{\hat{x} - number}{SE(\hat{x})}$$ When $\hat{s}$ is the difference between two averages, say, $\bar{x}_A$ and $\bar{x}_B$, the t-statistic is $$t = \frac{\bar{x}_A - \bar{x}_B}{SE(\bar{x}_A - \bar{x}_B)}$$

The sign of the t-statistic is the same as the sign of $\hat{s}$. If $\hat{s}$ is positive, the t-statistic is positive; if $\hat{s}$ is negative, the t-statistic is negative. The magnitude of the t-statistic measures the distance of $\hat{s}$ from what $s_{true}$ would be if the null were true. The unit of distance is the standard error.

## Making a Decision; False Negatives, False Positives

In hypothesis testing the decision of rejecting the null or not rejecting it is based on a clear rule specified in advance. To be specific, the decision rule in statistical testing is comparing the test statistic to a __critical value__. The critical value thus tell us whether the test statistic is large enough to reject the null. To be transparent, we need to set the critical value before looking at the test statistic, but the practice is to use a commonly accepted critical value. The critical value reflects a preference for how conservative we want to be with the evidence.

We can make one of two decisions with hypothesis testing: we reject the null or we don't reject the null. That decision may be right or wrong. We say that our decision is a __false positive__ (Type-I error) if we reject the null when it is true, and a __false negative__ (Type-II error) if we do not reject the null even though it's not true. False positives and false negatives are both wrong, but they are not equally wrong. The way we set up the null and the alternative, wrongly rejecting the null is a bigger mistake than wrongly accepting it. Therefore, the critical value is to be chosen in a way that makes false positives rare.

Talking about a t-test for a null with a two-side alternative, a commonly applied critical value for a t-statistic is $\pm2$: reject the null if the t-statistic is smaller than -2 or larger than +2; don't reject the null if the t-statistic is between -2 and +2. That way the probability of a false positive is 5%. Based on the same argument, we can set other critical values that correspond to different probabilities of a false positive. If we make the critical values -2.6 and +2.6, the chance of a false positive is 1%. With critical values -1.6 and +1.6 it is 10%. Data analysts avoid biases using the same $\pm2$ critical value of the t-test regardless of the data and hypothesis they are testing.

The chance of a false negative depends on two things: how far the true value is from the value defined in the null hypothesis, and how large the sample is. The probability of a false positive is called the __size of the test__. The maximum probability that we tolerate is the __level of significance__. When we fix the level of significance at 5% and end up rejecting the null, we say that the statistic we tested is significant at 5%. The probability of avoiding a false negative is the __power of the test__. Thus, we usually fix the level of significance at 5% and hope for a high power, which is more likely the larger the sample and the further away the true value is from what's in the null. Tests with more observations are more power in general.

## The p-Value

The __p-value__ informs us about the probability of a _false positive_. To be more precise, the p-value is the probability that the test statistic will be as large as, or larger than, what we calculate from the data, _if the null hypothesis is true_. Thus, the p-value is the smallest significance level at which we can reject $H_0$ given the value of the test statistic in the data. Because the p-value tells us the smallest level of significance at which we can reject the null, it summarizes all the information we need to make the decision.

```{r t-statistic}
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
t.test(pd$diff, mu = 0)
```
Conclusion: The t-statistic (0.4) is well within $\pm2$, thus we can't reject the hypothesis that the average price difference is zero in the population of products represented by the data. Doing so would be inconsistent with our goal to keep the probability of false positive below 5%. 

We arrive at the same conclusion with the help of the p-value (0.7). That means that there is a 66% chance that we could observe a price difference of this magnitude (or a larger one) if the null is true. It also means that the smallest level of significance at which we could reject the null is 66%, well over the 5% level of significance we have chosen.

Finally, we would arrive to a similar conclusion if we just looked at the 95% CI of the average price difference. The CI contains zero. So the value of the true average price difference may very well be zero. It also shows that it's very unlikely that the true average price is more than +0.30 or -0.19 cents.


## Steps of Hypothesis Testing

1. Define the __statistic__ that corresponds to your question.
2. Initial assessment of __external validity__: general pattern of interest, general pattern represented by the data.
3. State the __null__ and __alternative__ hypotheses.
4. Choose a __critical value__. In practice, that should correspond to 5%.
5. Calculate the __test statistic__ from the data.
6. Make a decision based on the calculated test statistic and the critical value.
7. Alternative to steps 4-6, calculate the __p-value__ and make a decision by comparing it to a pre-set level of significance that in practice will usually be 5%.
8. Interpret the results.
9. Final assessment of external validity. Would the same decision be fine for the population, or general pattern, of interest

## One-Sided Alternatives

Testing a null hypothesis against a one-sided alternative means having an inequality in the null hypothesis instead of an equality: $H_0:s_{true}  \le 0$ against $H_1:s_{true} \gt 0$ and $H_0:s_{true}  \ge 0$ against $H_1:s_{true} \lt 0$. Having only one of the inequalities leads to focusing on one side of the test statistic only. Focusing on deviations in one direction means that we care about one half of the sampling distribution of the test statistic.

The probability of a false positive is smaller in this case. We don't reject the null if the test statistic falls in the region that is specified in the null hypothesis. Thus, we make a false positive decision only half of the time. The probability of a false positive decision here is therefore half of the probability of a false positive in the corresponding two-sided test.

Therefore, the practical way to test a one-sided hypothesis is a two-step procedure.

1. If the test statistic is in the region of the null, don't reject the null. This happens if $\hat s$ is in the region of null ($H_0:s_{true}  \le 0$ against $H_1:s_{true} \gt 0$).
2. If the test statistic is in the region of the alternative, calculate the p-value of the null hypothesis of the equality and divide the p-value by two.

## Testing Multiple Hypotheses

The simplest decision rule to testing a hypothesis consists of calculating a p-value from the data and comparing it to a pre-set level of significance. So far, we have considered cases with one pair of hypotheses: a null and an alternative. Often, though, data analysis involves testing multiple null hypotheses, each with and an alternative. Testing multiple hypotheses is an issue because the statistical theory of testing, and the corresponding tools, have been developed for testing a single null hypothesis. In multiple hypothesis testing, the question is the likelihood that any one of the true nulls will be wrongfully rejected.

```{r testing-multiple-hypotheses}
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
# calculating multiple hypotheses for each retailer in the data (16)
spd <- split(pd, pd$retailer, drop = FALSE)
out <- vector("list", length = length(spd))
out <- lapply(1:length(spd), function(x) out[[x]] <- t.test(spd[[x]]$diff, mu = 0))
out

# creating a table with the 16 p-values if we carry out each test one by one
table_out <- pd |>
  group_by(retailer) |>
  group_modify(~ tidy(t.test(.x$diff)))
table_out <- table_out |>
  dplyr::select(retailer, estimate, p.value)
table_out
```
# CHAPTER 07: Simple Regression

We focus in the most important pattern of association: __mean dependence__. Mean-dependence tells whether, and to what extent, the mean of _y_ is different when the value of _x_ is different. __Regression__ is the most widely used method of comparison in data analysis. __Simple regression analysis__ uncovers mean-dependence between two variables. It amounts to comparing average values of one variable, called the dependent variable _(y)_ for observations that are different in the other variable, the explanatory variable _(x)_. Discovering patterns of association between variables if often a good starting point if we are interested in __causal analysis__: uncovering the effect of one variable on another variable; or, we may be interested in __predictive analysis__: what to expect of a variable for various values of another variable.

__Regression analysis__ is a method that uncovers the average value of a variable _y_ for different values of variable _x_. Regression is a model for the __conditional mean__. The model for the conditional mean gives a rule that takes _x_ values and tells what the mean value of _y_ is. $$E[y|x] = f(x) *  \\ y^E = f(x) \\ y = f(x) + e **$$

\* The expected value of _y_ conditional on _x_ is given by the function _f_.

\** The actual value of _y_ is equal to its expected value plus a deviation from it. 

Regression analysis find patterns in the data by comparing observations with different values of the _x_ variable __(explanatory variable, right-hand-side variable or regressor)__ to see whether and how the mean value of the _y_ variable __(explained variable, left-hand-side variable or regressand)__ differs across them. Thus, we need data with variation in _x_. Usually, the more variation in _x_, the better because it allows for more comparisons.

## Non-parametric Regression

__Non-parametric regressions__ describe the $y^E = f(x)$ pattern without imposing a specific __functional form__ of _f_. They let the data dictate what that function looks like. In contrast, __parametric regressions__ impose a functional form on _f_. Parametric examples include linear functions: $f(x) = a + bx$.

When _x_ has few values and there are many observations in the data, the best and most intuitive non-parametric regression shows average _y_ for each and every value of _x_. With many _x_ values and few observations for some, or all, _x_ values, there are two ways to do non-parametric regression analysis: __bins__ and __smoothing__. The first and more straightforward way is based on grouped values of _x_: bins. Bins are disjoint categories (no overlap) that span the entire range of _x_ (no gaps). It can be visualized through __bin scatters__. Bin scatters show the average value of _y_ that corresponds to each bin created from the values of _x_, in an _x-y_ coordinate system. An alternative visualization of the same non-parametric regression is the __step function__. A step function shows average _y_ over the entire range of the bins.

The second method produces a "smooth" graph that is both continuous and has no kink at any point. Non-parametric regression of this kind shows conditional means, smoothed to get a better image. One of the various methods available is called __lowess__. __Lowess__ (locally weighted scatterplot smoothing) is one of the most widely non-parametric regression methods that produces a smooth graph. A lowess may be thought of as a smooth curve fit around a bin scatter. Similarly to density plots, we have to set the bandwidth for smoothing, a choice that affects the results. Smooth non-parametric regression methods do not produce numbers that would summarize $y^E = f(x)$ pattern. Instead, they provide a value of $y^E$ for each of the particular _x_ values that occur in the data, as well as for all _x_ values in-between. We interpret these graphs in qualitative, not quantitative ways. If we want to visualize a regression, it is a good practice to plot it __together with the scatterplot__.

```{r simple-regression}
# CASE STUDY: Finding a Good Deal among Hotels with Simple Regression
hotels <- read_csv("input/hotels-vienna.csv")

# Apply filters:  3-4 stars, Vienna actual, without extreme value
hotels <- hotels |>
  filter(accommodation_type == "Hotel") |>
  filter(city_actual == "Vienna") |>
  filter(stars >= 3 & stars <= 4) |>
  filter(!is.na(stars)) |>
  filter(price <= 600)

# summary statistics on price 
descr_price <- hotels |>
  dplyr::select(price) |>
  dplyr::summarize(
    mean = mean(price),
    sd = sd(price),
    min = min(price),
    max = max(price),
    p50 = quantile(price, .50),
    p95 = quantile(price, .95),
    n = length(price)
  )
descr_price

# summary statistics on distance 
descr_dist <- hotels |>
  dplyr::select(distance) |> 
  dplyr::summarize(mean=mean(distance),
                  sd=sd(distance),
                  min=min(distance),
                  max=max(distance),
                  p50=quantile(distance,.50),
                  p95=quantile(distance,.95),
                  n=length(distance))
descr_dist

# REGRESSION 1: CLOSE VS FAR REGRESSION WITH BINARY DISTANCE
hotels <- hotels |> mutate(dist2 = as.numeric(distance >= 2))
dist2 <- hotels |>
  group_by(dist2) |>
  dplyr::summarize(Eprice_cat2 = mean(price))
hotels <- left_join(hotels, dist2)
hotels <- hotels |> mutate(dist2 = recode(dist2, `0` = "Close", `1` = "Far"))

hotels |>
  group_by(dist2) |>
  dplyr::summarize(
    mean_dist = mean(distance),
    sd_dist = sd(distance),
    min_dist = min(distance),
    max_dist = max(distance),
    mean_price = mean(price),
    sd_price = sd(price),
    min_price = min(price),
    max_price = max(price),
    N = n()
  )

# plotting the regression analysis (2 bins) with bin scatters
p1 <- ggplot(data = hotels) +
  geom_point(aes(x = dist2, y = Eprice_cat2),
    size = 2.5, color = color[1], fill = color[1], shape = 21, alpha = 0.4, na.rm = T
  ) +
  geom_text(aes(x = dist2, y = Eprice_cat2, label = round(Eprice_cat2)), hjust = -0.8, vjust = 0, color = "black", size = 3) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  expand_limits(y = 0.01) +
  scale_x_discrete() +
  labs(x = "Distance to city center (categories)", y = "Average price (US dollars)") +
  theme_bg()
p1

# plotting the box plot
p2 <- ggplot(data = hotels, aes(x = dist2, y = price)) +
  stat_boxplot(aes(group = dist2), geom = "errorbar", width = 0.25, color = viridis(2, begin = 0.3, end = 0.7), size = 0.5, na.rm = T) +
  geom_boxplot(aes(group = dist2), color = viridis(2, begin = 0.3, end = 0.7), fill = viridis(2, begin = 0.3, end = 0.7), size = 0.5, width = 0.5, alpha = 0.3, na.rm = T, outlier.shape = NA) +
  geom_jitter(aes(color = dist2), position = position_jitter(0.1), size = 0.5, show.legend = F, na.rm = T) +
  labs(x = "Distance to city center (categories)", y = "Price (US dollars)") +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, 50)) +
  expand_limits(y = 0.01) +
  scale_color_viridis(discrete = TRUE, option = "D", begin = 0.3, end = 0.7) +
  theme_bg()
p2

p1 | p2  

### REGRESSION 2: 4 DISTANCE CATEGORIES
hotels <- hotels |> mutate(dist4 = 0.5 + 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 2.5 * as.numeric(hotels$distance >= 3))
dist4 <- hotels |>
  group_by(dist4) |>
  dplyr::summarize(Eprice_cat4 = mean(price))
hotels <- left_join(hotels, dist4)

hotels |>
  group_by(dist4) |>
  dplyr::summarize(
    mean_dist = mean(distance),
    sd_dist = sd(distance),
    min_dist = min(distance),
    max_dist = max(distance),
    mean_price = mean(price),
    sd_price = sd(price),
    min_price = min(price),
    max_price = max(price),
    N = n()
  )

# plotting the regression analysis (4 bins) with bin scatters
p3 <- ggplot(data = hotels) +
  geom_point(aes(x = dist4, y = Eprice_cat4),
    size = 2.5, color = color[1], fill = color[1], shape = 21, alpha = 0.4, na.rm = T
  ) +
  geom_text(aes(x = dist4, y = Eprice_cat4, label = round(Eprice_cat4)), hjust = -0.6, vjust = 0, color = "black", size = 3) +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(xlim = c(0, 7), ylim = c(0, 400)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = c(0, 1, 2, 3, 4, 5, 6, 7)) +
  labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
  theme_bg()
p3

p1 | p3

# scatterplot of the whole data set (207 observations)
p4 <- ggplot(data = hotels) +
  geom_point(aes(x = distance, y = price), color = color[1], size = 2, shape = 16, alpha = 0.5, show.legend = F, na.rm = TRUE) +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = seq(0, 7, by = 1)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
  theme_bg()
p4

# non-parametric regression with a step function (4 bins)
hotels <- hotels |> mutate(dist4_s = 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
hotels$xend <- c(hotels$dist4_s + 1)
hotels$yend <- c(hotels$Eprice_cat4)

# plotting non-parametric regression including scatterplot (4 bins)
p5 <- p4 +
  geom_segment(data = hotels, aes(x = dist4_s, y = yend, xend = xend, yend = yend), color = color[2], size = 0.7, na.rm = TRUE)
p5

### REGRESSION 3: 7 DISTANCE CATEGORIES
hotels <- hotels |> mutate(dist7_new = 0.5 + 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
dist7_new <- hotels |>
  group_by(dist7_new) |>
  dplyr::summarize(Eprice_cat7_new = mean(price))
hotels <- left_join(hotels, dist7_new)
hotels |>
  group_by(dist7_new) |>
  dplyr::summarize(
    mean_dist = mean(distance),
    sd_dist = sd(distance),
    min_dist = min(distance),
    max_dist = max(distance),
    mean_dist = mean(price),
    sd_dist = sd(price),
    min_dist = min(price),
    max_dist = max(price),
    N = n()
  )

# non-parametric regression with a step function (7 bins)
hotels <- hotels %>% mutate(dist7_s = 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
hotels$xend <- c(hotels$dist7_s + 1)
hotels$yend <- c(hotels$Eprice_cat7_new)

# plotting non-parametric regression including scatterplot (7 bins)
p6 <- p4 +
  geom_segment(data = hotels, aes(x = dist7_s, y = yend, xend = xend, yend = yend), color = color[2], size = 0.7, na.rm = TRUE)
p6

p5 | p6

# LOWESS NONPARAMETRIC REGRESSION
p7 <- p4 +
  geom_smooth(aes(x = distance, y = price), method = "loess", se = FALSE)
p7
```

## Linear Regression

Simple linear regression imposes linearity of the function _f_ in $y^E = f(x)$. Linear functions have two parameters called coefficients: the __intercept coefficient__ and the __slope coefficient__. $$y^E = a + \beta x$$ To be more precise, linear regression imposes linearity in terms of its coefficients. We can have any function, including any nonlinear function, of the original variables themselves, and it will be still a linear regression as long as it consists of an intercept plus a slope coefficient, whether that slope multiplies _x_ or any function of _x_.

Linear regression is a line through the _x-y_ scatterplot. This line is the best-fitting line one can draw through the scatterplot. It is the best fit in the sense that it is the line that is closest to all points of the scatterplot.

One way to look at the linearity in linear regression is as an assumption: by doing linear regression analysis we assume that the regression function is linear in its coefficients. Another way to look at linearity is to treat it as an approximation. Whatever the form of the $y^E = f(x)$ relationship, the $y^E = a + \beta x$ regression fits a line through it. By fitting a line, linear regression approximates the average slope of the $y^E = f(x)$ curve: it is the difference in average _y_ that corresponds to different values of _x_, averaged across the entire range of _x_ in the data.

## Linear Regression: Coefficient Interpretation

What makes linear regression very powerful is that its coefficients have a clear interpretation based on the idea of comparing conditional means. The __intercept__ ($a$) is the average value of _y_ when _x_ is zero. The __slope__ ($\beta$) shows the expected difference in _y_ corresponding to a one unit difference in _x_. _y_ is higher, on average, by $\beta$ for observations with a one-unit higher value of _x_. 

## Linear Regression with a Binary Explanatory Variable

An important special case of linear regression is when _x_ is a binary variable, zero or one. In this case, the simple linear regression has the following interpretations:

* $a$ (intercept): the average value of _y_ is $a$ when _x_ is zero.
* $\beta$ (slope): average _y_ is $\beta$ higher among observations with $x = 1$ than among observations with $x = 0$.
* When $x = 1$, the average value of _y_ is $a + \beta$.

## Coefficient Formula

One of the advantages of linear regression is that there are ready formulae for its coefficients. At a practical level, we simply have to plug in appropriate values in the data in these formulae to get the coefficients.

$a$ and $\beta$ denote the coefficients of linear regression in the abstract. The calculated values $\hat{a}$ and $\hat\beta$ are called __estimates__ of the coefficients $a$ and $\beta$. The formulae are called __estimators__ as they give the estimates once data is plugged into them. 

The slope coefficient is calculated by first calculating the covariance of the dependent variable and the explanatory variable in the data and then dividing that by the variance of the explanatory variable in the data: $$\hat{\beta} = \frac{Cov[x, y]}{Var[x]}$$ One way to understand the slope coefficient formula is to view it as a normalized version of the covariance between _x_ and _y_. The slope measures the covariance relative to the variation in _x_. That is why the slope can be interpreted as differences in average _y_ corresponding to differences in _x_.

The intercept is calculated after the slope is calculated. It is average _y_ minus average _x_ multiplied by the estimated slope $\hat{\beta}$: $$\hat{a} = \bar{y} - \hat{\beta}\bar{x}$$ The formula of the intercept reveals that the regression line always goes through the point of average _x_ and average _y_. To see that, just rearrange the formula to get $$\bar{y} = \hat{a} + \hat{\beta}\bar{x}$$ In linear regressions, the expected value of _y_ for average _x_ is indeed average _y_. The derivation of the formulae is called __ordinary least squares (OLS)__. Data analysts sometimes use the name OLS for the linear regression with coefficients estimated by ordinary least squares. The linear regression is the best-fitting line through the scatterplot when its coefficients are estimated with OLS. Mathematically, the OLS method finds the values of the coefficients of the linear regression that minimize the sum of squares of the difference between actual _y_ values and their values implied by the regression, $\hat{a} + \hat{\beta}\bar{x}$.

```{r linear-regression}
# CASE STUDY: Finding a Good Deal among Hotels with Simple Regression
# load data
hotels <- read_csv("input/hotels-vienna.csv")

# Apply filters:  3-4 stars, Vienna actual, without extreme value
hotels <- hotels |>
  filter(accommodation_type == "Hotel") |>
  filter(city_actual == "Vienna") |>
  filter(stars >= 3 & stars <= 4) |>
  filter(!is.na(stars)) |>
  filter(price <= 600)

# plotting scatterplot
p1 <- ggplot(data = hotels) +
  geom_point(aes(x = distance, y = price), color = color[1], size = 2, shape = 16, alpha = 0.5, show.legend = F, na.rm = TRUE) +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = seq(0, 7, by = 1)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
  theme_bg()
p1

# plotting linear regression + lines for averages
p2 <- p1 +
  geom_smooth(aes(x = distance, y = price), method = "lm", se = FALSE) +
  geom_vline(xintercept = mean(hotels$distance), color = color[3], lty = "dashed", size = 0.3) +
  geom_hline(yintercept = mean(hotels$price), color = color[3], lty = "dashed", size = 0.3)
p2
```
## Predicted Dependent Variable and Regression Residual

The __predicted value__ of the dependent variable is our best guess for its average value if we know the value of the explanatory variable. The primary use of the predicted value is in predictive analysis. The predicted value is straightforward to compute in a linear regression: $$\hat{y}_i = \hat{a} + \hat{\beta}x_i$$ The __residual__ is another important measure that we can compute from a regression. It is the difference between the actual value of the dependent variable for an observation and its predicted value: $$e_i = y_i - \hat{y}_i$$ The concept of the residual is general for all regressions, not only linear regression. The residual is always the difference between the actual dependent variable and the value implied by the regression: $y_i - \hat{y}_i$.

With estimated $\hat{a}$ and $\hat \beta$, the predicted value of _y_ can be computed for any value of _x_. This is possible for any _x_ value: not only those that correspond to an observation in our data but other values of _x_, too. Computing the predicted value of the dependent variable for non-existing values of the explanatory variable in-between existing values in the data is __interpolation__. Computing it for values of the explanatory variable that are outside its range is called __extrapolation__. In contrast with predicted _y_ values, residuals can be computed for existing observations only.

```{r linear-regression-predicted-values}
# CASE STUDY: Finding a Good Deal among Hotels with Simple Regression
# load data
hotels <- read_csv("input/hotels-vienna.csv")

# Apply filters:  3-4 stars, Vienna actual, without extreme value
hotels <- hotels |>
  filter(accommodation_type == "Hotel") |>
  filter(city_actual == "Vienna") |>
  filter(stars >= 3 & stars <= 4) |>
  filter(!is.na(stars)) |>
  filter(price <= 600)

# calculating predicted values and the residuals
regression <- lm(price ~ distance, data = hotels)
hotels$predprice <- predict(regression)
hotels$e <- resid(regression)

# values for annotation
xa<- 2.9
ya<- 208
ym<- 90.24

# plotting the regression line and the residual
p1 <- ggplot(data = hotels, aes(x = distance, y = price)) +
  geom_point_da() +
  geom_smooth_da(method = "lm") +
  annotation_custom(grid.text("Residual", x = 0.48, y = 0.5, gp = gpar(color = "black", fontsize = 4, fontface = "bold"))) +
  annotate("pointrange", x = xa, y = ya, ymin = ya, ymax = ya, color = color[3], size = 0.1) +
  geom_errorbar(data = subset(hotels, hotels$distance == xa), aes(x = distance, ymin = ym, ymax = ya), width = 0.2, size = 0.2, color = color[1]) +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = seq(0, 7, by = 1)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
  theme_bg()
p1

# plotting histogram of the residuals
p2 <- ggplot(data = hotels, aes(x = e)) +
  geom_histogram(aes(y = (..count..) / sum(..count..)),
    binwidth = 20, color = color.outline, fill = theme_colors[1],
    size = 0.2, alpha = 0.8, show.legend = F, na.rm = TRUE, boundary = 1
  ) +
  labs(x = "Residuals", y = "Percent") +
  scale_x_continuous(limits = c(-100, 300), breaks = seq(-100, 300, by = 100)) +
  scale_y_continuous(
    expand = c(0.0, 0.0), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.05),
    labels = scales::percent_format(accuracy = 1)
  ) +
  theme_bg()
p2

# calculating predicted values and the residuals
reg1 <- lm(price ~ distance, data = hotels)
summary(reg1)

# creating new variables
hotels$reg1_resid <- reg1$residuals
hotels$reg1_res <- ifelse(reg1$residuals >= 0, "overpriced", "underpriced")
hotels$reg1_res <- ifelse(hotels$reg1_resid %in% tail(sort(reg1$residuals, decreasing = TRUE), 5), "bottom5",
  ifelse(hotels$reg1_resid %in% head(sort(reg1$residuals, decreasing = TRUE), 5), "top5", hotels$reg1_res)
)

# creating new data set
bestdeals <- hotels |>
  arrange(e)
bestdeals

# plotting underpriced hotels
p3 <- ggplot(data = hotels, aes(x = distance, y = price)) +
  geom_point(
    data = filter(hotels, reg1_res == "overpriced"), aes(color = factor(reg1_res)),
    size = 1.3, shape = 16, alpha = 0.6, show.legend = F
  ) +
  geom_point(
    data = filter(hotels, reg1_res == "underpriced"), aes(color = factor(reg1_res)),
    size = 1.3, shape = 16, alpha = 0.6, show.legend = F
  ) +
  geom_point(
    data = filter(hotels, reg1_res == "bottom5"), aes(color = factor(reg1_res), fill = factor(reg1_res)),
    size = 1.5, shape = 21, alpha = 0.8, show.legend = F
  ) +
  geom_point(
    data = filter(hotels, reg1_res == "top5"), aes(color = factor(reg1_res)),
    size = 1.3, shape = 16, alpha = 0.6, show.legend = F
  ) +
  geom_smooth_da(method = "lm", size = 1) +
  coord_cartesian(xlim = c(0, 7), ylim = c(0, 400)) +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = seq(0, 7, by = 1)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
  labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
  scale_color_manual(
    name = "",
    values = c(color[1], color[1], color[1], color[1])
  ) +
  scale_fill_manual(name = "", values = c(color[4])) +
  geom_segment(aes(x = 2, y = 25, xend = 1.15, yend = 50), arrow = arrow(length = unit(0.1, "cm"))) +
  annotate("text", x = 3, y = 25, label = "Most underpriced hotels", size = 3) +
  theme_bg() +
  theme(axis.text.x = element_text(size = 9)) +
  theme(axis.text.y = element_text(size = 9)) +
  theme(axis.title.x = element_text(size = 9)) +
  theme(axis.title.y = element_text(size = 9))
p3
```

# Linting

The code in this RMarkdown is linted with the [lintr package](https://github.com/jimhester/lintr), which is based on the [tidyverse style guide](http://style.tidyverse.org/).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
```
