---
title: "Data Analysis for Business, Economics and Policy"
subtitle: "Case studies"
author: "Fernando Millan Villalobos"
date: "July 2021"
output:
  html_document:
    code_folding: show
    echo: TRUE
    warning: FALSE
    message: FALSE
    highlight: pygments
    theme: paper
    df_print: kable
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float: 
      collapsed: yes
      smooth_scroll: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../", output_file = "index") })     
---

```{r, include=FALSE}
## By defult, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE
)
```

```{r, echo=FALSE}
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "dabep" # adapt!
package_date <- "2021-06-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.1.1" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
  stop("ERROR: specified R version does not match currently used.")
}
```

# Notes

This report was generated on `r Sys.time()`. R version: `r paste0(version$major, ".", version$minor)` on `r version$platform`. For this report, CRAN packages as of `r package_date` were used.

...

## R-Script & Data

The preprocessing and analysis of the hotels_raw was conducted in the [R project for statistical computing](https://www.r-project.org/). The RMarkdown script used to generate this document and all the resulting hotels_raw can be downloaded [under this link](http://%60r%20user_name%60.github.io/%60r%20project_name%60/). Through executing `main.Rmd`, the herein described process can be reproduced and this document can be generated. In the course of this, hotels_raw from the folder `input` will be processed and results will be written to `output`. The html on-line version of the analysis can be accessed through this [link](https://%60r%20user_name%60.github.io/%60r%20project_name%60/).

## GitHub

The code for the herein described process can also be freely downloaded from [https://github.com/`r user_name`/`r project_name`](https://github.com/%60r%20user_name%60/%60r%20project_name%60).

## License

...

## Data description of output files

#### `abc.csv` (Example)

| Attribute | Type    | Description |
|-----------|---------|-------------|
| a         | Numeric | ...         |
| b         | Numeric | ...         |
| c         | Numeric | ...         |

#### `xyz.csv`

...

# Set up

```{r, echo=FALSE}
detach_all_packages <- function() {
  basic_packages_blank <- c(
    "stats",
    "graphics",
    "grDevices",
    "utils",
    "hotels_rawsets",
    "methods",
    "base"
  )
  basic_packages <- paste("package:", basic_packages_blank, sep = "")

  package_list <- search()[
    ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)
  ]

  package_list <- setdiff(package_list, basic_packages)

  if (length(package_list) > 0) {
    for (package in package_list) {
      detach(package, character.only = TRUE, unload = TRUE)
      print(paste("package ", package, " detached", sep = ""))
    }
  }
}

detach_all_packages()

# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
  print("WARNING: No working directory specified for current user")
} else {
  setwd(path_to_wd)
}

# suppress scientific notation
options(scipen = 999)

# suppress summarise info
options(dplyr.summarise.inform = FALSE)

# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
  detach_all_packages()
}
```

## Define packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse, warn.conflicts = FALSE) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(jsonlite) # json
library(lintr) # code linting
library(sf)
library(rmarkdown)
library(data.table)
library(cowplot) # theme
library(extrafont)
library(waldo) # compare
library(psych) # some useful funs
library(ggrepel) # text labels
library(skimr)
library(haven)
library(Hmisc)
library(desc)
library(reshape2)
library(pastecs)
library(patchwork)
library(xtable)
library(reticulate)
library(janitor)", # names
  file = "manifest.R"
)
```

## Install packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
  if (!require(devtools)) {
    install.packages("devtools", repos = "http://cran.us.r-project.org")
    require(devtools)
  }
  devtools::install_github("RevolutionAnalytics/checkpoint",
    ref = "v0.3.2", # could be adapted later,
    # as of now (beginning of July 2017
    # this is the current release on CRAN)
    repos = "http://cran.us.r-project.org"
  )
  require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
  dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(
  snapshot_date = package_date,
  project = path_to_wd,
  verbose = T,
  scanForPackages = T,
  use.knitr = F,
  R.version = r_version
)
rm(package_date)
```

## Load packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
source("manifest.R")
unlink("manifest.R")
sessionInfo()
```

## Load additional scripts

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if you want to outsource logic to other script files, see README for
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
knitr::read_chunk("scripts/outliers.R")
source("scripts/outliers.R")
knitr::read_chunk("scripts/theme_bg.R")
source("scripts/theme_bg.R")
```

# CHAPTER 02: Preparing Data for Analysis

## Entity Resolution: Duplicates, Ambiguous Identification, and Non-entity Rows

One potential issue is having **duplicate observations** in the data table. In the simplest case, duplicates are perfect: the value of all variables is the same. In such cases we have to delete duplicates and leave one data row for each observation only.

A related, but conceptually different issue is to have **ambiguous identification**: the same entity having different IDs across different tables. The task here is to make sure that each entity has the same ID across data tables. That is necessary to link them properly. Entities are frequently identified by names. Unfortunately, though, names may cause issues for two main reasons: they are not unique, and different data tables may have different versions of the same name. This task is called **disambiguation**: making identification of entities not ambiguous.

Yet another issue is having non-entity observations: rows that do not belong to an entity we want in the data table. Before any meaningful analysis can be done, we need to erase such rows from the data table. The important message is this: *assigning unique IDs is important and using numerical ID variables is good practice*.

## Managing Missing Values

Having missing values for some variables is a frequent problem. There are two different main approaches: First, we can work with observations that have non-missing values for all variables used in the analysis. This is the most natural and most common approach. One version of this approach is to work with observations that make up a well-defined subsample, in which the missing data problem is a lot less severe. The second option is filling in some value for the missing values, such as the average value. This is called **imputation**. When missing values are replaced with some other values, it is good practice to create a binary variable that indicates that the original value was missing. Such a variable is called a **flag**.

When possible, focus on more fully filled variables. Imputation for qualitative variables should be done differently. We should add an additional category for missing values. For ordinal variables, we may also impute the median category and add a new variable denoting that the original value was missing.

## Organizing Data Tables for a Project

It is good practice to organize and store the data at three levels:

-   Raw data tables
-   Clean and tidy data tables
-   Workfile for analysis

**Raw data** may come in a single or in multiple files. It should be stored in the original format before anything is done to it. The next step is producing **clean** and **tidy** data from raw data. Often tidy data means multiple data tables. We should always create different data tables for data with different kinds of observations. The last of the three levels is the one on which the analysis is to be done: the **workfile**.

Project overview:

![Project Overview](input/project_organization.png)

```{r ch02_hotels}
# load hotels_raw
hotels_raw <- read_csv("input/hotelbookingdata-vienna.csv")
# load hotels_clean
hotels_clean <- read_csv("input/hotels-vienna.csv")

# transform values of some variables (through splitting) into valid values
hotels_raw <- hotels_raw |>
  tidyr::separate(center1distance, c("distance", NA), sep = " ") |>
  tidyr::separate(center2distance, c("distance_alter", NA), sep = " ") |>
  tidyr::separate(accommodationtype, c(NA, "accommodation_type"), sep = "@") |>
  tidyr::separate(price_night, c(NA, NA, "nnight", NA), sep = " ") |>
  tidyr::separate(guestreviewsrating, c("rating", NA), sep = " ")
head(hotels_raw)

# check: frequency table of all values incl. missing values
tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating_reviewcount <- hotels_raw |>
  dplyr::group_by(rating_reviewcount) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

hotels_raw <- hotels_raw |>
  dplyr::mutate(rating_count = as.numeric(rating_reviewcount))

Hmisc::describe(hotels_raw$rating_count)

# rename variables
hotels_raw <- hotels_raw |>
  dplyr::rename(
    ratingta = rating2_ta,
    ratingta_count = rating2_ta_reviewcount,
    country = addresscountryname,
    city = s_city, stars = starrating
  )


# check: key variables
tab_stars <- hotels_raw |>
  dplyr::group_by(stars) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

# look for perfect duplicates
hotels_raw <- hotels_raw |>
  dplyr::arrange(hotel_id)

# filtering duplicates rows by IDs and selecting key variables to check
hotels_raw |>
  dplyr::group_by(hotel_id) |>
  dplyr::filter(n() > 1) |>
  dplyr::select(c(hotel_id, accommodation_type, price, distance, stars, rating, rating_count))

# getting rid of all duplicates (perfect duplicates)
hotels_raw <- hotels_raw |>
  dplyr::distinct()

# handling missing values in text
# checking NAs values
summary(hotels_raw)

# making a summary of the missing values
summary_df <- t(stat.desc(hotels_raw))
summary_df

# creating a flag variable for missing values
hotels_raw <- hotels_raw |>
  dplyr::mutate(misrating = ifelse(is.na(rating), 1, 0))

# counting all missing values
table(hotels_raw$misrating)

# looking missing values number per accomodation type
addmargins(table(hotels_raw$accommodation_type, hotels_raw$misrating))

# looking at the relation between missing values and price (mean)
hotels_raw |>
  dplyr::group_by(accommodation_type, misrating) |>
  dplyr::summarise(mean(price))

# spotting the exceptional case for "Hotels"
hotels_raw |>
  dplyr::filter((misrating == 1) & (accommodation_type == "Hotel")) |>
  dplyr::select(hotel_id, accommodation_type, price, distance, stars, rating, rating_count) |>
  dplyr::slice(1)
```

## Finding the Most Successful Managers

```{r ch02_football}
# load data
games <- read_csv("input/epl_games.csv")
team_games <- read_csv("input/epl-teams-games.csv")
managers <- read_csv("input/football_managers.csv")
merged <- read_csv("input/football_managers_workfile.csv")

dplyr::glimpse(games)
dplyr::glimpse(team_games)
dplyr::glimpse(managers)
dplyr::glimpse(merged)

# look at basic data
games <- games |>
  dplyr::arrange(team_home)

games <- games |>
  dplyr::arrange(season, team_home)

games <- games |>
  dplyr::filter(season == 2016)

# at team-game level
team_games <- team_games |>
  dplyr::arrange(team)

team_games <- team_games |>
  dplyr::arrange(season, team)

team_games <- team_games |>
  dplyr::filter(season == 2016) |>
  dplyr::arrange(date)

Hmisc::describe(merged$manager_id)

# arranging per season and team
merged <- merged |>
  dplyr::arrange(season, team)

# considering average points per game as a measure of success
# 1. looking at the number of games managed per manager
games <- merged |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::summarise(manager_games = n())

# 2. adding up the points earned over a career at a team (if manager worked for two teams, we consider it two cases)
points <- merged |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::summarise(manager_points = sum(points))

# 3. once we have games and points, we divide total points by the number of games
avg_points <- merge(games, points, by = c("manager_id", "team", "manager_name")) |>
  dplyr::group_by(team, manager_id, manager_name) |>
  dplyr::mutate(manager_avg_points = (manager_points / manager_games)) |>
  dplyr::arrange(manager_avg_points)

# 4. ranking the results
avg_points <- avg_points |>
  dplyr::arrange(-manager_avg_points)

# 5. looking at those with at least 2 points per game
top_managers <- avg_points |>
  dplyr::filter(manager_avg_points >= 2)

# 6. denoting caretakers (less than 18 games)
top_managers <- top_managers |>
  dplyr::mutate(
    manager_avg_points0 = ifelse(manager_games < 18, manager_avg_points, NA),
    manager_avg_points1 = ifelse(manager_games > 18, manager_avg_points, NA)
  )

# adding new variable for plotting
top_managers <- top_managers |>
  dplyr::mutate(fill = case_when(
    manager_games < 18 ~ "1",
    manager_games > 18 ~ "0"
  ))

# plotting the results
p1 <- top_managers |>
  ggplot(aes(x = reorder(manager_name, manager_avg_points), y = manager_avg_points, fill = fill, alpha = fill)) +
  geom_col(show.legend = F) +
  ylab("Average points per game") +
  xlab("Manager name") +
  scale_fill_manual(values = c(color[1], color[4])) +
  scale_alpha_manual(values = c(0.8, 0.3)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 3), breaks = seq(0, 3, 0.3)) +
  coord_flip() +
  theme_bg() +
  cowplot::background_grid(major = "x", minor = "none")
p1
```

# CHAPTER 03: Exploratory Data Analysis (EDA)

## Frequencies and Probabilities
The __absolute frequency__, or count, of a value of a variable is simply the number of observations with that particular value in the data. The __relative frequency__ is the frequency expressed in relative, or percentage, terms: the proportion of observations with that particular value among all observations in the data. If a variable has missing values, this proportion can be relative to all observations including the missing values or only to observations with missing values.

__Probability__ is a general concept that is closely related to relative frequency. Probability is a measure of the likelihood of an event. In the context of data, an event is _the occurrence of a particular value of a variable_. __The probability of an event is its relative frequency.__ Probabilities are always between zero and one. We denote the probability of an event as $$P(event)$$, so that $$0 <= P(event) <= 1$$. Considering a single event, it either happens or does not. These two are mutually exclusive: the probability of an event happening and it also not happening is zero: $$P(event&~even) = 0$$.

## Visualizing Distributions
Frequencies are summarized by distributions. The __distribution__ of a variable gives the frequency of each value of the variable in the data, either in terms of absolute frequency (number of observations), or relative frequency (proportion or percent).

The simplest way to visualize a distribution is the __histogram__. For variables with many potential values, showing bars for each value is usually uninformative. Instead, it's better to group the many values in fewer groupings or bins. Visual inspection of a histogram can inform us about the number and location of __modes__: these are peaks in the distribution that stand out from their immediate neighborhood.

__Density plots__, also called __kernel density estimates__, are an alternative to histograms for variables with many values.

## Extreme Values
What it should be do with observations with extreme values of a variable depends on the role of the variable in our analysis. We distinguish a _y_ variable and one or more _x_ variables in the analysis. Typically, our analysis will aim to uncover patterns in how values of _y_ tend to differ for observations that have different values of _x_. __Discarding observations with extreme _x_ values narrows the scope of the comparisons.__ In contrast, discarding observations with extreme _y_ values changes the result of the comparisons.

```{r distributions}
# load data
vienna <- read_csv("input/hotels-vienna.csv")

# looking at the frequencies of all types of accommodation
table(vienna$accommodation_type)

# filtering accommodation: Hotel
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel")

# plotting distributions
# Absolute frequency (count)
p1 <- ggplot(data = vienna_hotels, aes(x = stars)) +
  geom_bar(color = color.outline, fill = color[1], alpha = 0.8, na.rm = T) +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 2.5) +
  labs(x = "Star rating (N. stars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0.5, 5.5), breaks = seq(1, 5, 0.5)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 140), breaks = seq(0, 140, 20)) +
  theme_bg()

# Relative frequency (percent)
p2 <- ggplot(data = vienna_hotels, aes(x = stars, y = (..count..) / sum(..count..))) +
  geom_bar(color = color.outline, fill = color[1], alpha = 0.8, na.rm = T) +
  geom_text(stat = "count", aes(label = round((..count..) / sum(..count..) * 100, 1)), vjust = -0.5, size = 2.5) +
  labs(x = "Star rating (N. stars)", y = "Percent") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0.5, 5.5), breaks = seq(1, 5, 0.5)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.5), breaks = seq(0, 0.5, 0.1), labels = scales::percent_format(accuracy = 1)) +
  theme_bg()

p1|p2

# filtering 3-4 stars, without 1000 euro extreme value
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(price <= 1000)

table(vienna_hotels$city)
table(vienna_hotels$stars)

# plotting hotel price (different bindwidth)
p3 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 10) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p4 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 40) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p5 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 80) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

p6 <- ggplot(data = vienna_hotels, aes(x = price)) +
  geom_histogram_da(type = "frequency", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 50)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 40), breaks = seq(0, 40, by = 5)) +
  theme_bg()

(p3 | p4 | p5) / p6

# plotting distance
p7 <- ggplot(data = vienna_hotels, aes(x = distance)) +
  geom_histogram_da(type = "frequency", binwidth = 0.5) +
  labs(x = "Distance to city center (miles)", y = "Frequency") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 14), breaks = seq(0, 14, by = 2)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 61), breaks = seq(0, 60, by = 10)) +
  geom_segment(aes(x = 8.2, y = 0, xend = 8.2, yend = 60), color = color[2], size = 1) +
  # geom_segment(aes(x = 10, y = 40, xend = 8.4, yend = 40), arrow = arrow(length = unit(0.2, "cm")))+
  annotate("text", x = 11, y = 29, label = "Too far out", size = 2) +
  annotate("rect", xmin = 8.2, xmax = 14, ymin = 0, ymax = 60, fill = color[4], alpha = 0.1) +
  theme_bg()
p7

# looking at the actual place of hotels (included outside the city of Vienna)
table(vienna_hotels$city_actual)

# filtering 3-4 stars, less than 8miles from center, without 1000 euro extreme value
vienna_hotels <- vienna |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(price <= 1000) |> 
  dplyr::filter(city_actual == "Vienna")
```

## Summary Statistics for Quantitative Variables
While measures of central value (such as mean, median) and spread (such as range, standard deviation) are usually well known, summary statistics that measure __skewness__ are less frequently used. A distribution may be skewed in two ways, having a long left tail or having a long right tail. A long left tail means having a few observations with large values with most observations having larger values. A long right tail means having a few observations with large values with most observations having smaller values. The statistic of skewness compares the mean and the median and is called the __mean-median measure of skewness__. When the distribution is symmetric, its mean and median are the same. When it is skewed with a long right tail, the mean is larger than the median. Conversely, when a distribution is skewed with a long left tail, the mean is smaller than the median. The mean-median measure of skewness captures this intuition. In order to make this measure comparable across various distributions, we use a standardized measure, dividing the difference by the standard deviation: 
$$ Skewness = \frac{\bar{x} - median[x]}{Std[x]}$$ 

```{r summary_statistics}
# CASE STUDY: Comparing Hotel Prices in Europe: Vienna vs London
# load data
hotels_europe_price <- read_csv("input/hotels-europe_price.csv")
hotels_europe_features <- read_csv("input/hotels-europe_features.csv")

# creating a working data table for analysis
hotels_europe <- dplyr::left_join(hotels_europe_price, hotels_europe_features, by = "hotel_id")

# filtering for same Vienna data we used + London same date
hotels_europe_london_viena <- hotels_europe |>
  dplyr::filter(year == 2017 & month == 11 & weekend == 0) |>
  dplyr::filter(city %in% c("Vienna", "London")) |>
  dplyr::filter(accommodation_type == "Hotel") |>
  dplyr::filter(stars >= 3 & stars <= 4) |>
  dplyr::filter(!is.na(stars)) |>
  dplyr::filter(city_actual %in% c("Vienna", "London")) |>
  dplyr::filter(price <= 600)

# looking at the mean price for both cities
hotels_europe_london_viena |>
  dplyr::group_by(city) |>
  dplyr::summarise(mean_price = mean(price), max = max(price), n = n())

# plotting the distribution of hotel price
p1 <- ggplot(data = filter(hotels_europe_london_viena, city == "Vienna"), aes(x = price)) +
  geom_histogram_da(type = "percent", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Percent", title = "Vienna") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.1), labels = scales::percent_format()) +
  theme_bg()

p2 <- ggplot(data = filter(hotels_europe_london_viena, city == "London"), aes(x = price)) +
  geom_histogram_da(type = "percent", binwidth = 20) +
  labs(x = "Price (US dollars)", y = "Percent", title = "London") +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.3), breaks = seq(0, 0.3, by = 0.1), labels = scales::percent_format()) +
  theme_bg()

(p1 | p2)

# plotting same data using density plot
p3 <- ggplot(data = hotels_europe_london_viena, aes(x = price, y = stat(density), color = city)) +
  geom_line(stat = "density", show.legend = F, na.rm = TRUE) +
  labs(x = "Price (US dollars)", y = "Density", color = "") +
  scale_color_manual(
    name = "",
    values = c(color[2], color[1]),
    labels = c("London", "Vienna")
  ) +
  scale_y_continuous(expand = c(0.0, 0.0), limits = c(0, 0.015), breaks = seq(0, 0.015, by = 0.003)) +
  scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 500), breaks = seq(0, 500, by = 100)) +
  geom_text(aes(x = 340, y = 0.0026, label = "London"), color = color[2], size = 2.5) +
  geom_text(aes(x = 170, y = 0.008, label = "Vienna"), color = color[1], size = 2.5) +
  theme_bg()
p3

# summarizing all statistics in one table
hotels_europe_london_viena_tab <- hotels_europe_london_viena |>
  dplyr::group_by(city) |>
  dplyr::summarise(
    n = length(price), mean = mean(price), median = median(price), min = min(price), max = max(price),
    sd = sd(price), skew = ((mean(price) - median(price)) / sd(price))
  )
hotels_europe_london_viena_tab

# CASE STUDY: Measuring Home Team Advantage in Football
# load data
games <- read_csv("input/epl_games.csv")

# looking at 2016/17 season only
games <- subset(games, season == 2016)

# adding a goal difference variable
games <- games |>
  dplyr::mutate(home_goaladv = goals_home - goals_away)

# summary statistics
summary(games$home_goaladv)
psych::describe(games$home_goaladv)

# plotting the distribution of the goal difference variable
p1 <- ggplot(data = games, aes(x = home_goaladv, y = (..count..) / sum(..count..))) +
  geom_histogram(
    color = color.outline, fill = theme_colors[1],
    size = 0.2, alpha = 0.8, show.legend = F, na.rm = TRUE,
    binwidth = 1
  ) +
  geom_text(stat = "count", aes(label = round((..count..) / sum(..count..) * 100, 1)), hjust = 0.5, vjust = -0.5, size = 2) +
  labs(x = "Goal difference", y = "Share of games (percent)") +
  scale_x_continuous(expand = c(0.05, 0.05), limits = c(-6, 6), breaks = seq(-6, 6, by = 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.25), breaks = seq(0, 0.25, by = 0.05), labels = scales::percent_format(accuracy = 5L)) +
  theme_bg()
p1
```
### Tables
To produce a good table, one needs to to think about its __usage__, choose __encoding__ and __scaffolding__ accordingly, and may add __annotation__. One important type wants to communicate the main result, or results, of the analysis. A table of this type is called a __result table__, or a communication table. Good communication tables are focused on one message. The other main table type is the __documentation table__. Its aim is to document exploratory data analysis. Documentation tables describe the structure of the data, one or more variables, or some other features such as missing values or extreme values. Such tables are also used to summarize the results of data cleaning and restructuring processes by showing numbers of observations and statistics of important variables for the original data and the data we chose to work with. 

Encoding here means what numbers to present, and in what detail. Documentation tables tend to be large and include everything that is, or may become, important. In contrast, communication tables should be simple and focused. A good practice of encoding is to show totals together with components. In a documentation table with numbers of observations in subgroups in the data, the total number of observations is usually included. Another good practice is to include the number of observations in all tables. The second question of encoding is how much detail the numbers should have in the table. Communication tables should have no more detail than necessary. Usually, documentation tables have numbers in more detail.

```{python summary_statistics_table}
# We calculated the mean, the standard deviation of the goal difference and the relative frequency of games

# importing the library
import pandas as pd

# converting R object to a python object
df = r.games
df.shape

# calculating the goal difference
df["home_goaladv"] = df["goals_home"] - df["goals_away"]
df.head()

# making a table to present the results
pd.DataFrame.from_dict(
    {
        "Statistics": [
            "Mean",
            "Standard deviation",
            "Percent positive",
            "Percent zero",
            "Percent negative",
            "Number of observations",
        ],
        "Value": [
            df["home_goaladv"].describe()["mean"],
            df["home_goaladv"].describe()["std"],
            (df["home_goaladv"] > 0).sum() / df["home_goaladv"].shape[0] * 100,
            (df["home_goaladv"] == 0).sum() / df["home_goaladv"].shape[0] * 100,
            (df["home_goaladv"] < 0).sum() / df["home_goaladv"].shape[0] * 100,
            df["home_goaladv"].describe()["count"],
        ],
    }
).round(1)
```

# Linting

The code in this RMarkdown is linted with the [lintr package](https://github.com/jimhester/lintr), which is based on the [tidyverse style guide](http://style.tidyverse.org/).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
```
