---
title: "hotels_raw Analysis for Business, Economics and Policy"
subtitle: "Case studies"
author: "Fernando Millan Villalobos"
date: "July 2021"
output:
  html_document:
    code_folding: show
    echo: TRUE
    warning: FALSE
    message: FALSE
    highlight: pygments
    theme: paper
    df_print: kable
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float: 
      collapsed: yes
      smooth_scroll: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../", output_file = "index") })     
---

```{r, include=FALSE}
## By defult, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
   echo=TRUE, message=FALSE, warning=FALSE
)
```

```{r, echo=FALSE}
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "dabep" # adapt!
package_date <- "2021-06-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.1.0" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
  stop("ERROR: specified R version does not match currently used.")
}
```

# Notes

This report was generated on `r Sys.time()`. R version: `r paste0(version$major, ".", version$minor)` on `r version$platform`. For this report, CRAN packages as of `r package_date` were used.

...

## R-Script & hotels_raw

The preprocessing and analysis of the hotels_raw was conducted in the [R project for statistical computing](https://www.r-project.org/). The RMarkdown script used to generate this document and all the resulting hotels_raw can be downloaded [under this link](http://%60r%20user_name%60.github.io/%60r%20project_name%60/). Through executing `main.Rmd`, the herein described process can be reproduced and this document can be generated. In the course of this, hotels_raw from the folder `input` will be processed and results will be written to `output`. The html on-line version of the analysis can be accessed through this [link](https://%60r%20user_name%60.github.io/%60r%20project_name%60/).

## GitHub

The code for the herein described process can also be freely downloaded from [https://github.com/`r user_name`/`r project_name`](https://github.com/%60r%20user_name%60/%60r%20project_name%60).

## License

...

## hotels_raw description of output files

#### `abc.csv` (Example)

| Attribute | Type    | Description |
|-----------|---------|-------------|
| a         | Numeric | ...         |
| b         | Numeric | ...         |
| c         | Numeric | ...         |

#### `xyz.csv`

...

# Set up

```{r, echo=FALSE}
detach_all_packages <- function() {
  basic_packages_blank <-  c("stats",
                             "graphics",
                             "grDevices",
                             "utils",
                             "hotels_rawsets",
                             "methods",
                             "base")
  basic_packages <- paste("package:", basic_packages_blank, sep = "")

  package_list <- search()[
    ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]

  package_list <- setdiff(package_list, basic_packages)

  if (length(package_list) > 0)  for (package in package_list) {
    detach(package, character.only = TRUE, unload = TRUE)
    print(paste("package ", package, " detached", sep = ""))
  }
}

detach_all_packages()

# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
  print("WARNING: No working directory specified for current user")
} else {
  setwd(path_to_wd)
}

# suppress scientific notation
options(scipen = 999)

# suppress summarise info
options(dplyr.summarise.inform = FALSE)

# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
  detach_all_packages()
}
```

## Define packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse, warn.conflicts = FALSE) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(jsonlite) # json
library(lintr) # code linting
library(sf) # spatial hotels_raw handling
library(rmarkdown)
library(data.table)
library(cowplot) # theme
library(extrafont)
library(waldo) # compare
library(psych) # some useful funs 
library(ggrepel) # text labels
library(skimr) # hotels_raw quality 
library(tidyverse)
library(haven)
library(Hmisc)
library(desc)
library(reshape2)
library(pastecs)
library(janitor)", # names
file = "manifest.R")
```

## Install packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
  if (!require(devtools)) {
    install.packages("devtools", repos = "http://cran.us.r-project.org")
    require(devtools)
  }
  devtools::install_github("RevolutionAnalytics/checkpoint",
                           ref = "v0.3.2", # could be adapted later,
                           # as of now (beginning of July 2017
                           # this is the current release on CRAN)
                           repos = "http://cran.us.r-project.org")
  require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
  dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(snapshot_date = package_date,
           project = path_to_wd,
           verbose = T,
           scanForPackages = T,
           use.knitr = F,
           R.version = r_version)
rm(package_date)
```

## Load packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
source("manifest.R")
unlink("manifest.R")
sessionInfo()
```

## Load additional scripts

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if you want to outsource logic to other script files, see README for 
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
knitr::read_chunk("scripts/outliers.R")
source("scripts/outliers.R")
knitr::read_chunk("scripts/theme_bg.R")
source("scripts/theme_bg.R")
```

# CHAPTER 02: Preparing Data for Analysis

## Entity Resolution: Duplicates, Ambiguous Identification, and Non-entity Rows

One potential issue is having **duplicate observations** in the data table. In the simplest case, duplicates are perfect: the value of all variables is the same. In such cases we have to delete duplicates and leave one data row for each observation only.

A related, but conceptually different issue is to have **ambiguous identification**: the same entity having different IDs across different tables. The task here is to make sure that each entity has the same ID across data tables. That is necessary to link them properly. Entities are frequently identified by names. Unfortunately, though, names may cause issues for two main reasons: they are not unique, and different data tables may have different versions of the same name. This task is called **disambiguation**: making identification of entities not ambiguous.

Yet another issue is having non-entity observations: rows that do not belong to an entity we want in the data table. Before any meaningful analysis can be done, we need to erase such rows from the data table. The important message is this: *assigning unique IDs is important and using numerical ID variables is good practice*.

## Managing missing values

Having missing values for some variables is a frequent problem. There are two different main approaches: First, we can work with observations that have non-missing values for all variables used in the analysis. This is the most natural and most common approach. One version of this approach is to work with observations that make up a well-defined subsample, in which the missing data problem is a lot less severe. The second option is filling in some value for the missing values, such as the average value. This is called **imputation**. When missing values are replaced with some other values, it is good practice to create a binary variable that indicates that the original value was missing. Such a variable is called a **flag**.

When possible, focus on more fully filled variables. Imputation for qualitative variables should be done differently. We should add an additional category for missing values. For ordinal variables, we may also impute the median category and add a new variable denoting that the original value was missing.

## Organizing Data Tables for a Project

It is good practice to organize and store the data at three levels:

-   Raw data tables
-   Clean and tidy data tables
-   Workfile for analysis

**Raw data** may come in a single or in multiple files. It should be stored in the original format before anything is done to it. The next step is producing **clean** and **tidy** data from raw data. Often tidy data means multiple data tables. We should always create different data tables for data with different kinds of observations. The last of the three levels is the one on which the analysis is to be done: the **workfile**.

Project overview:

![Project Overview](input/project_organization.png)

```{r ch02_hotels}
# load hotels_raw
hotels_raw <- read_csv("input/hotelbookingdata-vienna.csv")
# load hotels_clean
hotels_clean <- read_csv("input/hotels-vienna.csv")

# transform values of some variables (through splitting) into valid values
hotels_raw <- hotels_raw |>
  tidyr::separate(center1distance, c("distance", NA), sep = " ") |>
  tidyr::separate(center2distance, c("distance_alter", NA), sep = " ") |>
  tidyr::separate(accommodationtype, c(NA, "accommodation_type"), sep = "@") |>
  tidyr::separate(price_night, c(NA, NA, "nnight", NA), sep = " ") |>
  tidyr::separate(guestreviewsrating, c("rating", NA), sep = " ")
head(hotels_raw)

# check: frequency table of all values incl. missing values
tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating

tab_rating_reviewcount <- hotels_raw |>
  dplyr::group_by(rating_reviewcount) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating_reviewcount

hotels_raw <- hotels_raw |> 
  dplyr::mutate(rating_count = as.numeric(rating_reviewcount))

Hmisc::describe(hotels_raw$rating_count)

# rename variables
hotels_raw <- hotels_raw |> 
  dplyr::rename(
  ratingta = rating2_ta, 
  ratingta_count = rating2_ta_reviewcount,
  country = addresscountryname, 
  city = s_city, stars = starrating
)


# check: key variables
tab_stars <- hotels_raw |>
  dplyr::group_by(stars) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_stars

tab_rating <- hotels_raw |>
  dplyr::group_by(rating) |>
  dplyr::summarise(n = n()) |>
  dplyr::mutate(
    percent = round((n / sum(n)), 3),
    cumpercent = round(cumsum(freq = n / sum(n)), 3)
  )

tab_rating

# look for perfect duplicates
hotels_raw <- hotels_raw |> 
  dplyr::arrange(hotel_id)

# filtering duplicates rows by IDs and selecting key variables to check
hotels_raw |>
  dplyr::group_by(hotel_id) |>
  dplyr::filter(n() > 1) |> 
  dplyr::select(c(hotel_id, accommodation_type, price, distance, stars, rating, rating_count))

# getting rid of all duplicates (perfect duplicates)
hotels_raw <- hotels_raw |> 
  dplyr::distinct()

# handling missing values in text
# checking NAs values
summary(hotels_raw)

# making a summary of the missing values
summary_df <- t(stat.desc(hotels_raw))
summary_df

# creating a flag variable for missing values
hotels_raw <- hotels_raw |> 
  dplyr::mutate(misrating = ifelse(is.na(rating), 1, 0))

# counting all missing values
table(hotels_raw$misrating)

# looking missing values number per accomodation type
addmargins(table(hotels_raw$accommodation_type, hotels_raw$misrating))

# looking at the relation between missing values and price (mean)
hotels_raw |>
  dplyr::group_by(accommodation_type, misrating) |>
  dplyr::summarise(mean(price))

# spotting the exceptional case for "Hotels"
hotels_raw |>
  dplyr::filter((misrating == 1) & (accommodation_type == "Hotel")) |>
  dplyr::select(hotel_id, accommodation_type, price, distance, stars, rating, rating_count) |>
  dplyr::slice(1)

hotels_raw
hotels_clean
```

```{r ch02_football}
# load data
games <- read_csv("input/epl_games.csv")
team_games <- read_csv("input/epl-teams-games.csv")
managers <- read_csv("input/football_managers.csv")
merged <- read_csv("input/football_managers_workfile.csv")

dplyr::glimpse(games)
dplyr::glimpse(team_games)
dplyr::glimpse(managers)
dplyr::glimpse(merged)

# look at basic data
games <- games |>
  dplyr::arrange(team_home)
games

games <- games |>
  dplyr::arrange(season, team_home)
games

games <- games |>
 dplyr::filter(season == 2016)
games

# at team-game level
team_games <- team_games |>
  dplyr::arrange(team)
team_games

team_games <- team_games |>
  dplyr::arrange(season, team)

team_games <- team_games |>
  dplyr::filter(season == 2016) |>
  dplyr::arrange(date)
team_games

Hmisc::describe(merged$manager_id)

# finding the most successful managers
# arranging per season and team
merged <-  merged |>
  arrange(season, team)
merged

# considering average points per game as a measure of success
# 1. looking at the number of games managed per manager
games <- merged |>
  group_by(team, manager_id, manager_name) |>
  summarise(manager_games = n())
games

# 2. adding up the points earned over a career at a team (if manager worked for two teams, we consider it two cases)
points <- merged |>
  group_by(team, manager_id, manager_name) |>
  summarise(manager_points = sum(points))
points

# 3. once we have games and points, we divide total points by the number of games
avg_points <- merge(games, points, by = c("manager_id", "team", "manager_name")) |>
  group_by(team, manager_id, manager_name) |>
  mutate(manager_avg_points = (manager_points / manager_games)) |>
  arrange(manager_avg_points)
avg_points

# 4. ranking the results
avg_points <- avg_points |>
  arrange(-manager_avg_points)
avg_points

# 5. looking at those with at least 2 points per game
top_managers <-  avg_points |>
  filter(manager_avg_points >= 2)
top_managers

# 6. denoting caretakers (less than 18 games)
top_managers <- top_managers |>
  mutate(
    manager_avg_points0 = ifelse(manager_games < 18, manager_avg_points, NA),
    manager_avg_points1 = ifelse(manager_games > 18, manager_avg_points, NA)
  )
top_managers

# adding new variable for plotting
top_managers <- top_managers |>
  mutate(fill = case_when(
    manager_games < 18 ~ "1",
    manager_games > 18 ~ "0"
  ))
top_managers

# plotting the results
p1 <- top_managers |>
  ggplot(aes(x = reorder(manager_name, manager_avg_points), y = manager_avg_points, fill = fill, alpha = fill)) +
  geom_col(show.legend = F) +
  ylab("Average points per game") +
  xlab("Manager name") +
  scale_fill_manual(values = c(color[1], color[4])) +
  scale_alpha_manual(values = c(0.8, 0.3)) +
  scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 3), breaks = seq(0, 3, 0.3)) +
  coord_flip() +
  theme_bg() +
  cowplot::background_grid(major = "x", minor = "none")
p1
```
# Linting

The code in this RMarkdown is linted with the [lintr package](https://github.com/jimhester/lintr), which is based on the [tidyverse style guide](http://style.tidyverse.org/).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
```
