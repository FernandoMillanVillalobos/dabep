dplyr::group_by(industry_broad)  |>
dplyr::summarise(Mean = mean(management), Obs = n())
# adding correlation to the table
wms_tab$cor <- cor$COR
# recoding variables
wms_tab$industry_broad <- wms_tab$industry_broad  |>  dplyr::recode(
auto = "Auto",
chemicals_etc = "Chemicals",
electronics = "Machinery, equipment, electronics",
food_drinks_tobacco = "Food, drinks, tobacco",
materials_metals = "Materials, metals",
textile_apparel_leather_etc = "Textile, apparel, leather",
wood_furniture_paper = "Wood, furniture, paper",
other = "Other"
)
# adding the last row with all category
last_row <- wms_tab  |>  dplyr::summarise(Mean = mean(Mean), Obs = sum(Obs), cor = mean(cor))
last_row$industry_broad <- "All"
wms_tab <- wms_tab  |>  dplyr::add_row(
industry_broad = last_row$industry_broad,
Mean = last_row$Mean,
cor = last_row$cor,
Obs = last_row$Obs
)
# formatting the table
wms_tab <- wms_tab  |>  dplyr::select(industry_broad, cor, Mean, Obs)
wms_tab
# CASE STUDY: What likelihood of loss to expect on a stock portfolio
# load data
sp500 <- read_csv("input/SP500_2006_16_data.csv", na = c("", "#N/A"))
# subsetting all the values that are NOT NAs
sp500 <- subset(sp500, VALUE != "NA")
# creating percent return
sp500 <- sp500 |>
dplyr::mutate(pct_return = (VALUE - dplyr::lag(VALUE)) / dplyr::lag(VALUE) * 100)
# creating date variable
sp500$year <- format(sp500$DATE, "%Y")
sp500$month <- format(sp500$DATE, "%m")
sp500$year <- as.numeric(sp500$year)
sp500$month <- as.numeric(sp500$month)
sp500$yearmonth <- sp500$year * 100 + sp500$month
# plotting daily returns
p1 <- ggplot(sp500, aes(pct_return)) +
geom_histogram_da(binwidth = 0.25, type = "frequency") +
geom_vline(xintercept = -5, size = 0.7, color = color[2]) +
labs(x = "Daily return (percent)", y = "Frequency") +
coord_cartesian(xlim = c(-10, 10), ylim = c(0, 400)) +
scale_y_continuous(expand = c(0, 0)) +
geom_segment(aes(x = -6, y = 220, xend = -5, yend = 220), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = -8, y = 220, label = "5% loss", size = 2.5) +
theme_bg()
p1
# Create 10 000 samples, with 500 and 1000 observations in each sample, taken from sp500
# remove first row as it has NA in pct_return
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use.
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
samples_pcloss <- c()
for (i in 1:n_samples) {
single_sample <- sample(vector, n_obs, replace = FALSE)
samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
}
samples_pcloss
}
set.seed(123)
# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)
# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)
# calculating the se
se <- qnorm(0.975) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
# left <- mean(nobs_df$nobs_1000) - se
# right <- mean(nobs_df$nobs_1000) + se
# plotting simulated number of days with big losses
options(digits = 2)
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_500)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# comparing density for both simulations
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
ylab("Percent") +
xlab("Percent of days with losses over 5%") +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
scale_color_manual(name = "", values = c(color[2], color[1])) +
scale_fill_manual(name = "", values = c(color[2], color[1])) +
theme_bg() +
theme(
legend.position = c(0.7, 0.9),
legend.key.size = unit(x = 0.4, units = "cm"),
legend.direction = "horizontal"
)
p4
# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
headTail(sp500)
# CASE STUDY: What likelihood of loss to expect on a stock portfolio
# load data
sp500 <- read_csv("input/SP500_2006_16_data.csv", na = c("", "#N/A"))
# subsetting all the values that are NOT NAs
sp500 <- subset(sp500, VALUE != "NA")
# creating percent return
sp500 <- sp500 |>
dplyr::mutate(pct_return = (VALUE - dplyr::lag(VALUE)) / dplyr::lag(VALUE) * 100)
# creating date variable
sp500$year <- format(sp500$DATE, "%Y")
sp500$month <- format(sp500$DATE, "%m")
sp500$year <- as.numeric(sp500$year)
sp500$month <- as.numeric(sp500$month)
sp500$yearmonth <- sp500$year * 100 + sp500$month
# plotting daily returns
p1 <- ggplot(sp500, aes(pct_return)) +
geom_histogram_da(binwidth = 0.25, type = "frequency") +
geom_vline(xintercept = -5, size = 0.7, color = color[2]) +
labs(x = "Daily return (percent)", y = "Frequency") +
coord_cartesian(xlim = c(-10, 10), ylim = c(0, 400)) +
scale_y_continuous(expand = c(0, 0)) +
geom_segment(aes(x = -6, y = 220, xend = -5, yend = 220), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = -8, y = 220, label = "5% loss", size = 2.5) +
theme_bg()
p1
head(sp500)
View(sp500)
View(sp500)
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
pct_return
head(nobs_1000)
head(nobs_500)
str(nobs_1000)
class(nobs_500)
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use.
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
samples_pcloss <- c()
for (i in 1:n_samples) {
single_sample <- sample(vector, n_obs, replace = FALSE)
samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
}
samples_pcloss
}
set.seed(123)
# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)
# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)
str(nobs_1000)
class(nobs_500)
str(nobs_df)
nobs_df <- tibble(nobs_500,nobs_1000)
# calculating the se
se <- qnorm(0.975) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
# left <- mean(nobs_df$nobs_1000) - se
# right <- mean(nobs_df$nobs_1000) + se
# plotting simulated number of days with big losses
options(digits = 2)
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_500)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# calculating the se
se <- qnorm(0.975) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
se
p2
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
mean(nobs_df$nobs_500)
mean(nobs_df$nobs_1000)
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
ylab("Percent") +
xlab("Percent of days with losses over 5%") +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
scale_color_manual(name = "", values = c(color[2], color[1])) +
scale_fill_manual(name = "", values = c(color[2], color[1])) +
theme_bg() +
theme(
legend.position = c(0.7, 0.9),
legend.key.size = unit(x = 0.4, units = "cm"),
legend.direction = "horizontal"
)
p4
# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
se2 <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
se2
se
# calculating the se
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# Create 10 000 samples, with 500 and 1000 observations in each sample, taken from sp500
# remove first row as it has NA in pct_return
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use.
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
samples_pcloss <- c()
for (i in 1:n_samples) {
single_sample <- sample(vector, n_obs, replace = FALSE)
samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
}
samples_pcloss
}
set.seed(123)
# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)
# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)
# calculating the se
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
# left <- mean(nobs_df$nobs_1000) - se
# right <- mean(nobs_df$nobs_1000) + se
# plotting simulated number of days with big losses
options(digits = 2)
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# comparing density for both simulations
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
ylab("Percent") +
xlab("Percent of days with losses over 5%") +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
scale_color_manual(name = "", values = c(color[2], color[1])) +
scale_fill_manual(name = "", values = c(color[2], color[1])) +
theme_bg() +
theme(
legend.position = c(0.7, 0.9),
legend.key.size = unit(x = 0.4, units = "cm"),
legend.direction = "horizontal"
)
p4
# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
ci()
lsr::ciMean(nobs_1000)
lsr::ciMean(nobs_1000, conf = 0.95, na.rm = F)
nobs_df
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
left <- mean(nobs_df$nobs_1000) - se
right <- mean(nobs_df$nobs_1000) + se
left
right
lsr::ciMean(nobs_500, conf = 0.95)
# Create 10 000 samples, with 500 and 1000 observations in each sample, taken from sp500
# remove first row as it has NA in pct_return
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use.
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
samples_pcloss <- c()
for (i in 1:n_samples) {
single_sample <- sample(vector, n_obs, replace = FALSE)
samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
}
samples_pcloss
}
set.seed(123)
# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)
# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)
# calculating the se
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
# calculating CI
left <- mean(nobs_df$nobs_1000) - se
right <- mean(nobs_df$nobs_1000) + se
# plotting simulated number of days with big losses
options(digits = 2)
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# comparing density for both simulations
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
ylab("Percent") +
xlab("Percent of days with losses over 5%") +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
scale_color_manual(name = "", values = c(color[2], color[1])) +
scale_fill_manual(name = "", values = c(color[2], color[1])) +
theme_bg() +
theme(
legend.position = c(0.7, 0.9),
legend.key.size = unit(x = 0.4, units = "cm"),
legend.direction = "horizontal"
)
p4
# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
styler:::style_selection()
set.seed(573164)
M <- 10000
Results <- matrix(rep(0, (M * 10)), nrow = M, ncol = 10)
for (i in 1:M) {
bsample <- sample(sp500$pct_return, size = dim(sp500)[1], replace = TRUE)
for (j in 1:10) {
loss <- as.numeric(bsample < (-j)) * 100
Results[i, j] <- mean(loss, na.rm = T)
}
}
Results <- as_tibble(Results)
names(Results) <- c(
"loss1", "loss2", "loss3", "loss4", "loss5", "loss6",
"loss7", "loss8", "loss9", "loss10"
)
p1 <- ggplot(Results, aes(loss5)) +
geom_histogram_da(type = "frequency", binwidth = 0.04, boundary = 0, closed = "left") +
scale_y_continuous(expand = c(0, 0), limits = c(0, 1200), breaks = seq(0, 1200, 200)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.2), breaks = seq(0, 1.2, 0.1)) +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
theme_bg()
p1
Results
range(Results)
range(Results$loss2)
range(Results$loss8)
mean(Results)
mean(Results$loss1)
dim(Results$loss1)
glimpse(Results$loss1)
mean(Results[1, ] )
mean(Results[ ,1] )
head(Results)
head(nobs_500)
head(nobs_df)
Results$loss1
nobs_df
se <- qnorm(0.95) * sd(Results$loss1) / sqrt(length(Results$loss1))
se
se <- qnorm(0.95) * sd(Results$loss6) / sqrt(length(Results$loss6))
se
se <- qnorm(0.95) * sd(Results) / sqrt(length(Results))
View(Results)
View(Results)
lsr::ciMean(Results)
lsr::ciMean(Results$loss1)
lsr::ciMean(Results$loss9)
library(desc)
library(desc, lib.loc = "/usr/local/lib/R/site-library")
