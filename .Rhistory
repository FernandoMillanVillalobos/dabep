dplyr::group_by(emp10bins)  |>
dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))
wms_summary
# re-coding
levels(wms$emp10bins) <- wms_summary  |>
dplyr::pull(mean)  |>
round()
wms$emp10bins <- as.numeric(levels(wms$emp10bins))[wms$emp10bins]
# summary
wms  |>
dplyr::select(emp_firm, emp10bins)  |>
dplyr::group_by(emp10bins)  |>
dplyr::summarise_all(tibble::lst(min, max, mean, median, sd, length))
# generating variables by mean (10 bins)
wms_mean10 <- wms  |>
dplyr::group_by(emp10bins)  |>
dplyr::summarize(management_emp10bins = mean(management))
# plotting conditional mean (10 bins of employment)
p2 <- ggplot(data = wms_mean10, aes(x = emp10bins, y = management_emp10bins)) +
geom_point(size = 2, color = color[3], fill = color[1], shape = 21, alpha = 0.8, na.rm = T) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(2.5, 3.5), breaks = seq(2.5, 3.5, by = 0.25)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 3500), breaks = seq(0, 3500, by = 500)) +
labs(x = "Firm size (employment), 10 bins", y = "Management score") +
theme_bg()
p2
# this is a simpler solution, similar looking graph (not in book):
binsreg::binsreg(wms$management, wms$emp_firm, nbins = 3)
binsreg::binsreg(wms$management, wms$emp_firm, nbins = 10)
# plotting avg score by employment
p3 <- ggplot(data = wms, aes(x = emp_firm, y = management)) +
geom_point(color = color[1], size = 1.5, shape = 16, alpha = 0.8, show.legend = FALSE, na.rm = TRUE) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
labs(x = "Firm size (employment)", y = "Management score") +
theme_bg()
p3
# creating log var
wms$lnemp <-  log(wms$emp_firm)
# plotting avg score by employment (log)
p4 <- ggplot(data = wms, aes(x = lnemp, y = management)) +
geom_point(color = color[1], size = 1.5, shape = 16, alpha = 0.8, show.legend = FALSE, na.rm = TRUE) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(4, 9), breaks = seq(4, 9, by = 1)) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
labs(x = "Firm size (ln(employment))", y = "Management score") +
theme_bg()
p4
# transforming as bin categories factor
wms$emp3bins <- as.factor(wms$emp3bins)
levels(wms$emp3bins) <- c("Small", "Medium", "Large")
# plotting boxplots
p5 <- ggplot(data = wms, aes(x = emp3bins, y = management)) +
stat_boxplot(aes(group = emp3bins), geom = "errorbar", width = 0.5, color = c(color[2], color[1], color[3]), size = 0.5, na.rm = T) +
geom_boxplot(aes(group = emp3bins), color = c(color[2], color[1], color[3]), fill = c(color[2], color[1], color[3]), size = 0.5, width = 0.5, alpha = 0.3, na.rm = T) +
labs(x = "Firm size (employment), 3 bins", y = "Management score") +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(1, 5), breaks = seq(1, 5, 1)) +
theme_bg()
p5
# plotting violin plots
p6 <- ggplot(data = wms, aes(x = emp3bins, y = management, color = emp3bins, fill = emp3bins)) +
geom_violin(aes(group = emp3bins), size = 0.3, alpha = 0.3, trim = F, show.legend = F, na.rm = TRUE) +
geom_boxplot(aes(group = emp3bins), color = c(color[2], color[1], color[3]), fill = c(color[2], color[1], color[3]), size = 0.5, width = 0.2, alpha = 0.3, na.rm = T) +
labs(x = "Firm size (employment), 3 bins", y = "Management score") +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 6), breaks = seq(0, 6, 1)) +
scale_color_manual(
name = "",
values = c(color[2], color[1], color[3])
) +
scale_fill_manual(
name = "",
values = c(color[2], color[1], color[3])
) +
theme_bg()
p6
# CASE STUDY: Management quality and firm size: describing patterns of association
# calculating overall correlation coefficient
cor(wms$management, wms$emp_firm, use = "complete.obs")
# looking at the most recent industry code for the firms
table(wms$sic)
# recoding industry codes
wms$industry_broad[wms$sic <= 21] <- "food_drinks_tobacco"
wms$industry_broad[wms$sic >= 22 & wms$sic <= 23 | wms$sic == 31] <- "textile_apparel_leather_etc"
wms$industry_broad[wms$sic >= 24 & wms$sic <= 27] <- "wood_furniture_paper"
wms$industry_broad[wms$sic >= 28 & wms$sic <= 30] <- "chemicals_etc"
wms$industry_broad[wms$sic >= 32 & wms$sic < 35] <- "materials_metals"
wms$industry_broad[wms$sic >= 35 & wms$sic < 37] <- "electronics"
wms$industry_broad[wms$sic == 37] <- "auto"
wms$industry_broad[wms$sic >= 38] <- "other"
table(wms$industry_broad)
# summary management
wms  |>
dplyr::select(management, industry_broad)  |>
dplyr::filter(!is.na(industry_broad))  |>
dplyr::group_by(industry_broad)  |>
dplyr::summarise(
Min = min(management),
Max = max(management),
SD = sd(management),
Median = median(management),
n()
)
# summary employment
wms  |>
dplyr::select(emp_firm, industry_broad)  |>
dplyr::filter(!is.na(industry_broad))  |>
dplyr::group_by(industry_broad)  |>
dplyr::summarise(
Min = min(emp_firm),
Max = max(emp_firm),
SD = sd(emp_firm),
Median = median(emp_firm),
n()
)
# calculating correlation per industry broad
cor <- wms  |>
dplyr::group_by(industry_broad)  |>
dplyr::summarize(COR = cor(management, emp_firm))
# creating a table with the average of all relevant variables for the latent
wms_tab <- wms  |>
dplyr::select(emp_firm, industry_broad, management)  |>
dplyr::group_by(industry_broad)  |>
dplyr::summarise(Mean = mean(management), Obs = n())
# adding correlation to the table
wms_tab$cor <- cor$COR
# recoding variables
wms_tab$industry_broad <- wms_tab$industry_broad  |>  dplyr::recode(
auto = "Auto",
chemicals_etc = "Chemicals",
electronics = "Machinery, equipment, electronics",
food_drinks_tobacco = "Food, drinks, tobacco",
materials_metals = "Materials, metals",
textile_apparel_leather_etc = "Textile, apparel, leather",
wood_furniture_paper = "Wood, furniture, paper",
other = "Other"
)
# adding the last row with all category
last_row <- wms_tab  |>  dplyr::summarise(Mean = mean(Mean), Obs = sum(Obs), cor = mean(cor))
last_row$industry_broad <- "All"
wms_tab <- wms_tab  |>  dplyr::add_row(
industry_broad = last_row$industry_broad,
Mean = last_row$Mean,
cor = last_row$cor,
Obs = last_row$Obs
)
# formatting the table
wms_tab <- wms_tab  |>  dplyr::select(industry_broad, cor, Mean, Obs)
wms_tab
# CASE STUDY: What likelihood of loss to expect on a stock portfolio
# load data
sp500 <- read_csv("input/SP500_2006_16_data.csv", na = c("", "#N/A"))
# subsetting all the values that are NOT NAs
sp500 <- subset(sp500, VALUE != "NA")
# creating percent return
sp500 <- sp500 |>
dplyr::mutate(pct_return = (VALUE - dplyr::lag(VALUE)) / dplyr::lag(VALUE) * 100)
# creating date variable
sp500$year <- format(sp500$DATE, "%Y")
sp500$month <- format(sp500$DATE, "%m")
sp500$year <- as.numeric(sp500$year)
sp500$month <- as.numeric(sp500$month)
sp500$yearmonth <- sp500$year * 100 + sp500$month
# plotting daily returns
p1 <- ggplot(sp500, aes(pct_return)) +
geom_histogram_da(binwidth = 0.25, type = "frequency") +
geom_vline(xintercept = -5, size = 0.7, color = color[2]) +
labs(x = "Daily return (percent)", y = "Frequency") +
coord_cartesian(xlim = c(-10, 10), ylim = c(0, 400)) +
scale_y_continuous(expand = c(0, 0)) +
geom_segment(aes(x = -6, y = 220, xend = -5, yend = 220), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = -8, y = 220, label = "5% loss", size = 2.5) +
theme_bg()
p1
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
# Create 10 000 samples, with 500 and 1000 observations in each sample, taken from sp500
# remove first row as it has NA in pct_return
pct_return <- sp500  |>
dplyr::filter(!is.na(pct_return))  |>
dplyr::pull(pct_return)
# function for a specified number of samples: draws a specified number of observations from a vector, calculates the percentage of obs with greater than 5% losses
# 3 inputs: 'vector' is a vector of the source data, in this case pct_return. 'n_samples' is the number of samples we want to use.
# 'n_obs' is the number of observations in each sample
# output is a vector
create_samples <- function(vector, n_samples, n_obs) {
samples_pcloss <- c()
for (i in 1:n_samples) {
single_sample <- sample(vector, n_obs, replace = FALSE)
samples_pcloss[i] <- sum(single_sample < -5) / n_obs * 100
}
samples_pcloss
}
set.seed(123)
# creating samples
nobs_1000 <- create_samples(pct_return, 10000, 1000)
nobs_500 <- create_samples(pct_return, 10000, 500)
# converting results as tibble
nobs_df <- tibble(nobs_500,nobs_1000)
# calculating the se
se <- qnorm(0.95) * sd(nobs_df$nobs_1000) / sqrt(length(nobs_df$nobs_1000))
# calculating CI
left <- mean(nobs_df$nobs_1000) - se
right <- mean(nobs_df$nobs_1000) + se
# plotting simulated number of days with big losses
options(digits = 2)
p2 <- ggplot(nobs_df, aes(nobs_1000)) +
geom_histogram(binwidth = 0.1, color = color.outline, fill = color[1], alpha = 0.8, boundary = 0, closed = "left") +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
geom_vline(aes(xintercept = mean(nobs_1000)), color = color[2], size = 0.7) +
coord_cartesian(xlim = c(0, 1.5), ylim = c(0, 2500)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) +
geom_segment(aes(x = 0.8, y = 2000, xend = 0.53, yend = 2000), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 0.85, y = 2000, label = "Mean", size = 2.5) +
theme_bg()
p2
# comparing density for both simulations
p3 <- ggplot(nobs_df, aes(nobs_1000)) +
stat_density(geom = "line", aes(color = "n1000"), bw = 0.45, size = 1, kernel = "epanechnikov") +
stat_density(geom = "line", aes(nobs_500, color = "n500"), bw = 0.45, linetype = "twodash", size = 1, kernel = "epanechnikov") +
labs(x = "Percent of days with losses over 5%", y = "Density") +
geom_vline(xintercept = 0.5, colour = color[3], size = 0.7, linetype = "dashed") +
geom_segment(aes(x = 0.9, y = 0.72, xend = 0.65, yend = 0.72), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.72, label = "Larger sample", size = 2) +
geom_segment(aes(x = 0.9, y = 0.68, xend = 0.65, yend = 0.68), size = 0.5, arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 1.1, y = 0.68, label = "Smaller sample", size = 2) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 0.8), breaks = seq(0, 0.8, by = 0.2)) +
scale_color_manual(name = "", values = c(n1000 = color[1], n500 = color[2])) +
theme_bg() +
theme(legend.position = "none")
p3
# plotting histogram of big loss simulations with N = 500 and N = 1000
p4 <- ggplot(data = nobs_df) +
geom_histogram(aes(x = nobs_500, y = (..count..) / sum(..count..) * 100, color = "n500", fill = "n500"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.7) +
geom_histogram(aes(x = nobs_1000, y = (..count..) / sum(..count..) * 100, color = "n1000", fill = "n1000"), binwidth = 0.2, boundary = 0, closed = "left", alpha = 0.1, size = 0.7) +
ylab("Percent") +
xlab("Percent of days with losses over 5%") +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.2)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 50)) +
scale_color_manual(name = "", values = c(color[2], color[1])) +
scale_fill_manual(name = "", values = c(color[2], color[1])) +
theme_bg() +
theme(
legend.position = c(0.7, 0.9),
legend.key.size = unit(x = 0.4, units = "cm"),
legend.direction = "horizontal"
)
p4
# looking at distribution in a table
janitor::tabyl(nobs_df$nobs_500, sort = TRUE)
janitor::tabyl(nobs_df$nobs_1000, sort = TRUE)
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
set.seed(573164)
M <- 10000
Results <- matrix(rep(0, (M * 10)), nrow = M, ncol = 10)
for (i in 1:M) {
bsample <- sample(sp500$pct_return, size = dim(sp500)[1], replace = TRUE)
for (j in 1:10) {
loss <- as.numeric(bsample < (-j)) * 100
Results[i, j] <- mean(loss, na.rm = T)
}
}
Results <- as_tibble(Results)
names(Results) <- c(
"loss1", "loss2", "loss3", "loss4", "loss5", "loss6",
"loss7", "loss8", "loss9", "loss10"
)
p1 <- ggplot(Results, aes(loss5)) +
geom_histogram_da(type = "frequency", binwidth = 0.04, boundary = 0, closed = "left") +
scale_y_continuous(expand = c(0, 0), limits = c(0, 1200), breaks = seq(0, 1200, 200)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 1.2), breaks = seq(0, 1.2, 0.1)) +
labs(x = "Percent of days with losses of 5% or more", y = "Frequency") +
theme_bg()
p1
# CASE STUDY: What Likelihood of Loss to Expect on a Stock Portfolio?
# using the psych package to figure out the sd = 0.07
psych::describe(sp500$pct_return > -5)
# calculating the denominator of the SEM formula = 0.02
x <- sqrt((1 / 2519))
# calculating the product of both operations -> 0.02 * 0.07 = 0.0014 (0.14%) or the standard error of the mean
sem <- x * 0.07
sem
# calculating the 95% CI
right <- 0.5 + 2 * 0.14
left <- 0.5 - 2 * 0.14
c(left, right)
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
pd <- read.csv("input/online_offline_ALL_clean.csv")
# filter the data
pd <- pd |>
filter(COUNTRY == "USA") |>
filter(PRICETYPE == "Regular Price") |>
filter(is.na(sale_online)) |>
filter(!is.na(price)) |>
filter(!is.na(price_online))
# drop errors (+1000 dollars)
pd <- pd |> filter(price < 1000)
# creating diff variable
pd <- pd |> mutate(diff = price_online - price)
# Check the main descriptives
modelsummary::datasummary(diff ~ Mean + SD + Min + Max + Median + Max, data = pd)
# plotting online-offline price differences
p1 <- ggplot(data = pd, aes(diff)) +
geom_histogram(
binwidth = 5, boundary = 0, closed = "left",
fill = color[1], size = 0.25, alpha = 0.8, show.legend = F, na.rm = TRUE
) +
labs(x = "Online - offline price difference (US dollars)", y = "Frequency") +
theme_bg() +
scale_x_continuous(limits = c(-420, 420), breaks = seq(-400, 420, by = 100)) +
scale_y_continuous(limits = c(0, 6000), breaks = seq(0, 6000, by = 1000), expand = c(0.01, 0.01)) +
geom_segment(aes(x = 300, y = 500, xend = 415, yend = 20), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = 300, y = 700, label = "max value= 415", size = 2.5) +
geom_segment(aes(x = -280, y = 500, xend = -380, yend = 20), arrow = arrow(length = unit(0.1, "cm"))) +
annotate("text", x = -300, y = 700, label = "min value= -380", size = 2.5)
p1
# 4.99999 not 5 -- needed because of data imported from stata may be stored wierdly.
# pd1 <- subset(pd, abs(pd$diff) < 4.999999)
# Hmisc::describe(pd1$diff)
# plotting online-offline price differences (+-5 dollar price difference)
p2 <- ggplot(data = pd, aes(diff)) +
geom_histogram(
binwidth = 0.5, boundary = -0, closed = "left",
color = color.outline, fill = color[1], size = 0.25, alpha = 0.8, show.legend = F, na.rm = TRUE
) +
labs(x = "Online - offline price difference (US dollars)", y = "Frequency") +
theme_bg() +
expand_limits(x = 0.01, y = 0.01) +
scale_x_continuous(limits = c(-5, 5), breaks = seq(-5, 5, by = 1)) +
scale_y_continuous(expand = c(0.00, 0.00), limits = c(0, 5000), breaks = seq(0, 5000, by = 1000))
p2
p1 | p2
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
t.test(pd$diff, mu = 0)
# CASE STUDY: Comparing Online and Offline Prices: Testing the Difference
# calculating multiple hypotheses for each retailer in the data (16)
spd <- split(pd, pd$retailer, drop = FALSE)
out <- vector("list", length = length(spd))
out <- lapply(1:length(spd), function(x) out[[x]] <- t.test(spd[[x]]$diff, mu = 0))
out
# creating a table with the 16 p-values if we carry out each test one by one
table_out <- pd |>
group_by(retailer) |>
group_modify(~ tidy(t.test(.x$diff)))
table_out <- table_out |>
dplyr::select(retailer, estimate, p.value)
table_out
# CASE STUDY: Finding a Good Deal among Hotels with Simple Regression
hotels <- read_csv("input/hotels-vienna.csv")
# Apply filters:  3-4 stars, Vienna actual, without extreme value
hotels <- hotels |>
filter(accommodation_type == "Hotel") |>
filter(city_actual == "Vienna") |>
filter(stars >= 3 & stars <= 4) |>
filter(!is.na(stars)) |>
filter(price <= 600)
# summary statistics on price
descr_price <- hotels |>
dplyr::select(price) |>
dplyr::summarize(
mean = mean(price),
sd = sd(price),
min = min(price),
max = max(price),
p50 = quantile(price, .50),
p95 = quantile(price, .95),
n = length(price)
)
descr_price
# summary statistics on distance
descr_dist <- hotels |>
dplyr::select(distance) |>
dplyr::summarize(mean=mean(distance),
sd=sd(distance),
min=min(distance),
max=max(distance),
p50=quantile(distance,.50),
p95=quantile(distance,.95),
n=length(distance))
descr_dist
# REGRESSION 1: CLOSE VS FAR REGRESSION WITH BINARY DISTANCE
hotels <- hotels |> mutate(dist2 = as.numeric(distance >= 2))
dist2 <- hotels |>
group_by(dist2) |>
dplyr::summarize(Eprice_cat2 = mean(price))
hotels <- left_join(hotels, dist2)
hotels <- hotels |> mutate(dist2 = recode(dist2, `0` = "Close", `1` = "Far"))
hotels |>
group_by(dist2) |>
dplyr::summarize(
mean_dist = mean(distance),
sd_dist = sd(distance),
min_dist = min(distance),
max_dist = max(distance),
mean_price = mean(price),
sd_price = sd(price),
min_price = min(price),
max_price = max(price),
N = n()
)
# plotting the regression analysis (2 bins) with bin scatters
p1 <- ggplot(data = hotels) +
geom_point(aes(x = dist2, y = Eprice_cat2),
size = 2.5, color = color[1], fill = color[1], shape = 21, alpha = 0.4, na.rm = T
) +
geom_text(aes(x = dist2, y = Eprice_cat2, label = round(Eprice_cat2)), hjust = -0.8, vjust = 0, color = "black", size = 3) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
expand_limits(y = 0.01) +
scale_x_discrete() +
labs(x = "Distance to city center (categories)", y = "Average price (US dollars)") +
theme_bg()
p1
# plotting the box plot
p2 <- ggplot(data = hotels, aes(x = dist2, y = price)) +
stat_boxplot(aes(group = dist2), geom = "errorbar", width = 0.25, color = viridis(2, begin = 0.3, end = 0.7), size = 0.5, na.rm = T) +
geom_boxplot(aes(group = dist2), color = viridis(2, begin = 0.3, end = 0.7), fill = viridis(2, begin = 0.3, end = 0.7), size = 0.5, width = 0.5, alpha = 0.3, na.rm = T, outlier.shape = NA) +
geom_jitter(aes(color = dist2), position = position_jitter(0.1), size = 0.5, show.legend = F, na.rm = T) +
labs(x = "Distance to city center (categories)", y = "Price (US dollars)") +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, 50)) +
expand_limits(y = 0.01) +
scale_color_viridis(discrete = TRUE, option = "D", begin = 0.3, end = 0.7) +
theme_bg()
p2
p1 | p2
### REGRESSION 2: 4 DISTANCE CATEGORIES
hotels <- hotels |> mutate(dist4 = 0.5 + 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 2.5 * as.numeric(hotels$distance >= 3))
dist4 <- hotels |>
group_by(dist4) |>
dplyr::summarize(Eprice_cat4 = mean(price))
hotels <- left_join(hotels, dist4)
hotels |>
group_by(dist4) |>
dplyr::summarize(
mean_dist = mean(distance),
sd_dist = sd(distance),
min_dist = min(distance),
max_dist = max(distance),
mean_price = mean(price),
sd_price = sd(price),
min_price = min(price),
max_price = max(price),
N = n()
)
# plotting the regression analysis (4 bins) with bin scatters
p3 <- ggplot(data = hotels) +
geom_point(aes(x = dist4, y = Eprice_cat4),
size = 2.5, color = color[1], fill = color[1], shape = 21, alpha = 0.4, na.rm = T
) +
geom_text(aes(x = dist4, y = Eprice_cat4, label = round(Eprice_cat4)), hjust = -0.6, vjust = 0, color = "black", size = 3) +
expand_limits(x = 0.01, y = 0.01) +
coord_cartesian(xlim = c(0, 7), ylim = c(0, 400)) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = c(0, 1, 2, 3, 4, 5, 6, 7)) +
labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
theme_bg()
p3
p1 | p3
# scatterplot of the whole data set (207 observations)
p4 <- ggplot(data = hotels) +
geom_point(aes(x = distance, y = price), color = color[1], size = 2, shape = 16, alpha = 0.5, show.legend = F, na.rm = TRUE) +
expand_limits(x = 0.01, y = 0.01) +
scale_x_continuous(expand = c(0.01, 0.01), limits = c(0, 7), breaks = seq(0, 7, by = 1)) +
scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 400), breaks = seq(0, 400, by = 50)) +
labs(x = "Distance to city center (miles)", y = "Price (US dollars)") +
theme_bg()
p4
# non-parametric regression with a step function (4 bins)
hotels <- hotels |> mutate(dist4_s = 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
hotels$xend <- c(hotels$dist4_s + 1)
hotels$yend <- c(hotels$Eprice_cat4)
# plotting non-parametric regression including scatterplot (4 bins)
p5 <- p4 +
geom_segment(data = hotels, aes(x = dist4_s, y = yend, xend = xend, yend = yend), color = color[2], size = 0.7, na.rm = TRUE)
p5
### REGRESSION 3: 7 DISTANCE CATEGORIES
hotels <- hotels |> mutate(dist7_new = 0.5 + 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
dist7_new <- hotels |>
group_by(dist7_new) |>
dplyr::summarize(Eprice_cat7_new = mean(price))
hotels <- left_join(hotels, dist7_new)
hotels |>
group_by(dist7_new) |>
dplyr::summarize(
mean_dist = mean(distance),
sd_dist = sd(distance),
min_dist = min(distance),
max_dist = max(distance),
mean_dist = mean(price),
sd_dist = sd(price),
min_dist = min(price),
max_dist = max(price),
N = n()
)
# non-parametric regression with a step function (7 bins)
hotels <- hotels %>% mutate(dist7_s = 1 * as.numeric(hotels$distance >= 1) + 1 * as.numeric(hotels$distance >= 2) + 1 * as.numeric(hotels$distance >= 3) + 1 * as.numeric(hotels$distance >= 4) + 1 * as.numeric(hotels$distance >= 5) + 1 * as.numeric(hotels$distance >= 6))
hotels$xend <- c(hotels$dist7_s + 1)
hotels$yend <- c(hotels$Eprice_cat7_new)
# plotting non-parametric regression including scatterplot (7 bins)
p6 <- p4 +
geom_segment(data = hotels, aes(x = dist7_s, y = yend, xend = xend, yend = yend), color = color[2], size = 0.7, na.rm = TRUE)
p6
p5 | p6
# LOWESS NONPARAMETRIC REGRESSION
p7 <- p4 +
geom_smooth(aes(x = distance, y = price), method = "loess", se = FALSE)
p7
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
